apiVersion: v1
kind: Service
metadata:
  name: mysql
  namespace: aorist
spec:
  ports:
  - port: 3306
    targetPort: 3306
  selector:
    app: mysql
  clusterIP: None
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: mysql
  namespace: aorist
spec:
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: eagerLamprey
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
  namespace: aorist
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  selector:
    matchLabels:
      app: mysql
---
# Hive standalone metastore service, keeps track of table metadata.
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: metastore
  name: metastore
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: metastore
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: metastore
    spec:
      containers:
      - name: metastore
        image: scienz/aorist-test-metastore:latest
        env:
          - name: MYSQL_HOST
            value: mysql
          - name: METASTORE_PORT
            value: "9083"
          - name: MYSQL_USER
            value: root
          - name: MYSQL_PASSWORD
            value: "eagerLamprey"
          - name: MYSQL_PORT
            value: "3306"
          - name: METASTORE_DB
            value: "metastore_db"
        imagePullPolicy: "Always"
        ports:
        - containerPort: 9083
        resources: {}
        volumeMounts:
          - mountPath: /apache-hive-metastore-3.0.0-bin/metastore-site.xml.template
            name: properties-vol
            subPath: metastore-site.xml.template
            readOnly: false
      volumes:
        - name: properties-vol
          configMap:
            defaultMode: 420
            name: properties
      restartPolicy: Always
      serviceAccountName: ""
status: {}
---
# Hive Metastore service, exposes port 9083 to other services (Presto /
# Alluxio)
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: metastore
  name: metastore
  namespace: aorist
spec:
  ports:
  - port: 9083
  selector:
    io.kompose.service: metastore
status:
  loadBalancer: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: properties
  namespace: aorist
data:
  metastore-site.xml.template: |
    <configuration>
        <property>
            <name>metastore.thrift.uris</name>
            <value>thrift://0.0.0.0:9083</value>
            <description>
                Thrift URI for the remote metastore. Used by metastore client
                to connect to remote metastore.
            </description>
        </property>
        <property>
            <name>metastore.thrift.port</name>
            <value>9083</value>
        </property>
        <property>
            <name>metastore.task.threads.always</name>
            <value>org.apache.hadoop.hive.metastore.events.EventCleanerTask</value>
        </property>
        <property>
            <name>metastore.expression.proxy</name>
            <value>org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://$MYSQL_HOST:$MYSQL_PORT/$METASTORE_DB</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>$MYSQL_USER</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>$MYSQL_PASSWORD</value>
        </property>
    </configuration>
  alluxio-site.properties: |
    alluxio.master.mount.table.root.ufs=s3a://minio-test-bucket/
    alluxio.underfs.s3.endpoint=minio:9000
    alluxio.underfs.s3.disable.dns.buckets=true
    alluxio.underfs.s3.inherit.acl=false
    alluxio.tmp.dirs=/tmp
    aws.accessKeyId=minio
    aws.secretKey=minio123
    alluxio.user.file.writetype.default=THROUGH
    alluxio.proxy.s3.writetype=THROUGH
    alluxio.proxy.s3.deletetype=ALLUXIO_AND_UFS
    alluxio.master.hostname=$ALLUXIO_MASTER_HOSTNAME
    alluxio.master.journal.type=UFS
    alluxio.worker.memory.size=1000MB
    alluxio.underfs.s3.streaming.upload.enabled=false
    alluxio.user.file.metadata.load.type=ONCE
    alluxio.user.file.passive.cache.enabled=true
    alluxio.user.file.persistence.initial.wait.time=-1
    alluxio.user.short.circuit.enabled=true
  log4j.properties: |
    log4j.rootLogger=DEBUG, ${alluxio.logger.type}, ${alluxio.remote.logger.type}
    log4j.rootLogger=INFO, ${alluxio.logger.type}, ${alluxio.remote.logger.type}
    log4j.category.alluxio.logserver=INFO, ${alluxio.logserver.logger.type}
    log4j.additivity.alluxio.logserver=false
    log4j.logger.AUDIT_LOG=INFO, ${alluxio.master.audit.logger.type}
    log4j.additivity.AUDIT_LOG=false
    # Configures an appender whose name is "" (empty string) to be NullAppender.
    # By default, if a Java class does not specify a particular appender, log4j will
    # use "" as the appender name, then it will use Null appender.
    log4j.appender.=org.apache.log4j.varia.NullAppender
    log4j.appender.Console=org.apache.log4j.ConsoleAppender
    log4j.appender.Console.Target=System.out
    log4j.appender.Console.layout=org.apache.log4j.PatternLayout
    log4j.appender.Console.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # The ParquetWriter logs for every row group which is not noisy for large row group size,
    # but very noisy for small row group size.
    log4j.logger.org.apache.parquet.hadoop.InternalParquetRecordWriter=WARN
    log4j.logger.org.apache.parquet.hadoop.InternalParquetRecordReader=WARN
    # Appender for Job Master
    log4j.appender.JOB_MASTER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.JOB_MASTER_LOGGER.File=${alluxio.logs.dir}/job_master.log
    log4j.appender.JOB_MASTER_LOGGER.MaxFileSize=10MB
    log4j.appender.JOB_MASTER_LOGGER.MaxBackupIndex=100
    log4j.appender.JOB_MASTER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.JOB_MASTER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M) - %m%n
    # Appender for Job Workers
    log4j.appender.JOB_WORKER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.JOB_WORKER_LOGGER.File=${alluxio.logs.dir}/job_worker.log
    log4j.appender.JOB_WORKER_LOGGER.MaxFileSize=10MB
    log4j.appender.JOB_WORKER_LOGGER.MaxBackupIndex=100
    log4j.appender.JOB_WORKER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.JOB_WORKER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M) - %m%n
    # Appender for Master
    log4j.appender.MASTER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.MASTER_LOGGER.File=${alluxio.logs.dir}/master.log
    log4j.appender.MASTER_LOGGER.MaxFileSize=10MB
    log4j.appender.MASTER_LOGGER.MaxBackupIndex=100
    log4j.appender.MASTER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.MASTER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Appender for Master
    log4j.appender.SECONDARY_MASTER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.SECONDARY_MASTER_LOGGER.File=${alluxio.logs.dir}/secondary_master.log
    log4j.appender.SECONDARY_MASTER_LOGGER.MaxFileSize=10MB
    log4j.appender.SECONDARY_MASTER_LOGGER.MaxBackupIndex=100
    log4j.appender.SECONDARY_MASTER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.SECONDARY_MASTER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Appender for Master audit
    log4j.appender.MASTER_AUDIT_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.MASTER_AUDIT_LOGGER.File=${alluxio.logs.dir}/master_audit.log
    log4j.appender.MASTER_AUDIT_LOGGER.MaxFileSize=10MB
    log4j.appender.MASTER_AUDIT_LOGGER.MaxBackupIndex=100
    log4j.appender.MASTER_AUDIT_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.MASTER_AUDIT_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M) - %m%n
    # Appender for Proxy
    log4j.appender.PROXY_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.PROXY_LOGGER.File=${alluxio.logs.dir}/proxy.log
    log4j.appender.PROXY_LOGGER.MaxFileSize=10MB
    log4j.appender.PROXY_LOGGER.MaxBackupIndex=100
    log4j.appender.PROXY_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.PROXY_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Appender for Workers
    log4j.appender.WORKER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.WORKER_LOGGER.File=${alluxio.logs.dir}/worker.log
    log4j.appender.WORKER_LOGGER.MaxFileSize=10MB
    log4j.appender.WORKER_LOGGER.MaxBackupIndex=100
    log4j.appender.WORKER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.WORKER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Remote appender for Job Master
    log4j.appender.REMOTE_JOB_MASTER_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.filter.ID.ProcessType=JOB_MASTER
    log4j.appender.REMOTE_JOB_MASTER_LOGGER.Threshold=WARN
    # Remote appender for Job Workers
    log4j.appender.REMOTE_JOB_WORKER_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.filter.ID.ProcessType=JOB_WORKER
    log4j.appender.REMOTE_JOB_WORKER_LOGGER.Threshold=WARN
    # Remote appender for Master
    log4j.appender.REMOTE_MASTER_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_MASTER_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_MASTER_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_MASTER_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_MASTER_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_MASTER_LOGGER.filter.ID.ProcessType=MASTER
    log4j.appender.REMOTE_MASTER_LOGGER.Threshold=WARN
    # Remote appender for Secondary Master
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.filter.ID.ProcessType=SECONDARY_MASTER
    log4j.appender.REMOTE_SECONDARY_MASTER_LOGGER.Threshold=WARN
    # Remote appender for Proxy
    log4j.appender.REMOTE_PROXY_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_PROXY_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_PROXY_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_PROXY_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_PROXY_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_PROXY_LOGGER.filter.ID.ProcessType=PROXY
    log4j.appender.REMOTE_PROXY_LOGGER.Threshold=WARN
    # Remote appender for Workers
    log4j.appender.REMOTE_WORKER_LOGGER=org.apache.log4j.net.SocketAppender
    log4j.appender.REMOTE_WORKER_LOGGER.Port=${alluxio.logserver.port}
    log4j.appender.REMOTE_WORKER_LOGGER.RemoteHost=${alluxio.logserver.hostname}
    log4j.appender.REMOTE_WORKER_LOGGER.ReconnectionDelay=10000
    log4j.appender.REMOTE_WORKER_LOGGER.filter.ID=alluxio.AlluxioRemoteLogFilter
    log4j.appender.REMOTE_WORKER_LOGGER.filter.ID.ProcessType=WORKER
    log4j.appender.REMOTE_WORKER_LOGGER.Threshold=WARN
    # (Local) appender for log server itself
    log4j.appender.LOGSERVER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.LOGSERVER_LOGGER.File=${alluxio.logs.dir}/logserver.log
    log4j.appender.LOGSERVER_LOGGER.MaxFileSize=10MB
    log4j.appender.LOGSERVER_LOGGER.MaxBackupIndex=100
    log4j.appender.LOGSERVER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.LOGSERVER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # (Local) appender for log server to log on behalf of log clients
    # No need to configure file path because log server will dynamically
    # figure out for each appender.
    log4j.appender.LOGSERVER_CLIENT_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.LOGSERVER_CLIENT_LOGGER.MaxFileSize=10MB
    log4j.appender.LOGSERVER_CLIENT_LOGGER.MaxBackupIndex=100
    log4j.appender.LOGSERVER_CLIENT_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.LOGSERVER_CLIENT_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Appender for User
    log4j.appender.USER_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.USER_LOGGER.File=${alluxio.user.logs.dir}/user_${user.name}.log
    log4j.appender.USER_LOGGER.MaxFileSize=10MB
    log4j.appender.USER_LOGGER.MaxBackupIndex=10
    log4j.appender.USER_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.USER_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Appender for Fuse
    log4j.appender.FUSE_LOGGER=org.apache.log4j.RollingFileAppender
    log4j.appender.FUSE_LOGGER.File=${alluxio.logs.dir}/fuse.log
    log4j.appender.FUSE_LOGGER.MaxFileSize=10MB
    log4j.appender.FUSE_LOGGER.MaxBackupIndex=10
    log4j.appender.FUSE_LOGGER.layout=org.apache.log4j.PatternLayout
    log4j.appender.FUSE_LOGGER.layout.ConversionPattern=%d{ISO8601} %-5p %c{1} - %m%n
    # Disable noisy DEBUG logs
    log4j.logger.io.grpc.netty.NettyServerHandler=OFF
    # Disable noisy INFO logs from ratis
    log4j.logger.org.apache.ratis.grpc.server.GrpcLogAppender=ERROR
    log4j.logger.org.apache.ratis.grpc.server.GrpcServerProtocolService=WARN
    log4j.logger.org.apache.ratis.server.impl.FollowerInfo=WARN
    log4j.logger.org.apache.ratis.server.impl.RaftServerImpl=WARN
  hive.properties: |
    connector.name=hive-hadoop2
    hive.metastore.uri=thrift://metastore:9083
    hive.config.resources=/presto-server-344/etc/catalog/core-site.xml
    hive.temporary-staging-directory-enabled=false
    hive.s3.endpoint=s3a://minio:9000/
    hive.allow-add-column=true
    hive.allow-comment-column=true
    hive.allow-comment-table=true
    hive.allow-drop-column=true
    hive.allow-drop-table=true
    hive.allow-rename-column=true
    hive.allow-rename-table=true
  log.properties: |
    io.prestosql=INFO
  node.properties.template: |
    node.environment=production
    node.id=$(uuidgen -N $HOSTNAME --namespace @dns --md5)
    node.data-dir=/var/presto/data
  config.properties.template: |
    coordinator=$(bash -c "if [ $HOSTNAME == 'presto-coordinator-0' ]; then echo 'true'; else echo 'false'; fi")
    node-scheduler.include-coordinator=false
    http-server.http.port=8080
    query.max-memory=40GB
    query.max-memory-per-node=4000MB
    query.max-total-memory-per-node=4000MB
    task.writer-count=2
    discovery.uri=http://presto-coordinator-headless:8080
    $(bash -c "if [ $HOSTNAME == 'presto-coordinator-0' ]; then echo '
    discovery-server.enabled=true
    '; else echo ''; fi")
  core-site.xml: |
    <configuration>
      <property>
        <name>fs.alluxio.impl</name>
        <value>alluxio.hadoop.FileSystem</value>
      </property>
      <property>
        <name>fs.s3.awsAccessKeyId</name>
        <value>minio</value>
      </property>
      <property>
        <name>fs.s3.awsSecretAccessKey</name>
        <value>minio123</value>
      </property>
    </configuration>
  jvm.config: |
    -server
    -Xmx6G
    -XX:-UseBiasedLocking
    -XX:+UseG1GC
    -XX:G1HeapRegionSize=32M
    -XX:+ExplicitGCInvokesConcurrent
    -XX:+ExitOnOutOfMemoryError
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:ReservedCodeCacheSize=512M
    -XX:PerMethodRecompilationCutoff=10000
    -XX:PerBytecodeRecompilationCutoff=10000
    -Djdk.attach.allowAttachSelf=true
    -Djdk.nio.maxCachedBufferSize=2000000
    -Xbootclasspath/a:/presto-server-344/etc/alluxio-site.properties
  storage-schemas.conf: |
    [carbon]
    pattern = ^carbon\.
    retentions = 1s:1d,1min:90d
    [default]
    pattern = .*
    retentions = 1s:14d,1min:30d
  carbon.conf: |
    [cache]
    ENABLE_LOGROTATION = True
    USER =
    MAX_CACHE_SIZE = inf
    MAX_UPDATES_PER_SECOND = 50
    MAX_CREATES_PER_MINUTE = 500
    LINE_RECEIVER_INTERFACE = 0.0.0.0
    LINE_RECEIVER_PORT = 2003
    ENABLE_UDP_LISTENER = False
    UDP_RECEIVER_INTERFACE = 0.0.0.0
    UDP_RECEIVER_PORT = 2003
    PICKLE_RECEIVER_INTERFACE = 0.0.0.0
    PICKLE_RECEIVER_PORT = 2004
    LOG_LISTENER_CONNECTIONS = True
    USE_INSECURE_UNPICKLER = False
    CACHE_QUERY_INTERFACE = 0.0.0.0
    CACHE_QUERY_PORT = 7002
    USE_FLOW_CONTROL = True
    LOG_UPDATES = False
    LOG_CACHE_HITS = False
    LOG_CACHE_QUEUE_SORTS = True
    CACHE_WRITE_STRATEGY = sorted
    WHISPER_AUTOFLUSH = False
    WHISPER_FALLOCATE_CREATE = True

    [relay]
    LINE_RECEIVER_INTERFACE = 0.0.0.0
    LINE_RECEIVER_PORT = 2013
    PICKLE_RECEIVER_INTERFACE = 0.0.0.0
    PICKLE_RECEIVER_PORT = 2014
    LOG_LISTENER_CONNECTIONS = True
    RELAY_METHOD = rules
    REPLICATION_FACTOR = 1
    DESTINATIONS = 127.0.0.1:2004
    MAX_DATAPOINTS_PER_MESSAGE = 500
    MAX_QUEUE_SIZE = 10000
    USE_FLOW_CONTROL = True

    [aggregator]
    LINE_RECEIVER_INTERFACE = 0.0.0.0
    LINE_RECEIVER_PORT = 2023
    PICKLE_RECEIVER_INTERFACE = 0.0.0.0
    PICKLE_RECEIVER_PORT = 2024
    LOG_LISTENER_CONNECTIONS = True
    FORWARD_ALL = True
    DESTINATIONS = 127.0.0.1:2004
    REPLICATION_FACTOR = 1
    MAX_QUEUE_SIZE = 10000
    USE_FLOW_CONTROL = True
    MAX_DATAPOINTS_PER_MESSAGE = 500
    MAX_AGGREGATION_INTERVALS = 5
  graphite.yaml: |
    {
      "apiVersion": 1,
      "datasources": [
        {
          "name": "Benchto graphite",
          "type": "graphite",
          "url": "http://benchto-graphite:80",
          "access": "proxy",
          "isDefault": true,
          "basicAuth": true,
          "basicAuthUser": "guest",
          "basicAuthPassword": "guest"
        }
      ]
    }
  application-presto-devenv.yaml: |
    benchmark-service:
      url: http://benchto-service:8080

    data-sources:
      presto:
        url: jdbc:presto://presto-coordinator-headless:8080?user=test
        driver-class-name: io.prestosql.jdbc.PrestoDriver

    environment:
      name: PRESTO-DEVENV

    presto:
      url: http://presto-coordinator-headless:8080

    benchmark:
      feature:
        presto:
          metrics.collection.enabled: true

    macros:
      sleep-4s:
        command: echo "Sleeping for 4s" && sleep 4
  ranger-admin-install.properties.template: |
    #------------------------- DB CONFIG - BEGIN ----------------------------------
    # Uncomment the below if the DBA steps need to be run separately
    #setup_mode=SeparateDBA

    PYTHON_COMMAND_INVOKER=python3

    #DB_FLAVOR=MYSQL|ORACLE|POSTGRES|MSSQL|SQLA
    DB_FLAVOR=MYSQL
    #

    #
    # Location of DB client library (please check the location of the jar file)
    #
    #SQL_CONNECTOR_JAR=/usr/share/java/ojdbc6.jar
    #SQL_CONNECTOR_JAR=/usr/share/java/mysql-connector-java.jar
    #SQL_CONNECTOR_JAR=/usr/share/java/postgresql.jar
    #SQL_CONNECTOR_JAR=/usr/share/java/sqljdbc4.jar
    #SQL_CONNECTOR_JAR=/opt/sqlanywhere17/java/sajdbc4.jar
    SQL_CONNECTOR_JAR=/mysql-connector-java-8.0.19/mysql-connector-java-8.0.19.jar


    #
    # DB password for the DB admin user-id
    # **************************************************************************
    # ** If the password is left empty or not-defined here,
    # ** it will try with blank password during installation process
    # **************************************************************************
    #
    db_root_user=$MYSQL_USER
    db_root_password=$MYSQL_PASSWORD
    db_host=$MYSQL_HOST:$MYSQL_PORT
    #SSL config
    db_ssl_enabled=false
    db_ssl_required=false
    db_ssl_verifyServerCertificate=false
    #db_ssl_auth_type=1-way|2-way, where 1-way represents standard one way ssl authentication and 2-way represents mutual ssl authentication
    db_ssl_auth_type=2-way
    javax_net_ssl_keyStore=
    javax_net_ssl_keyStorePassword=
    javax_net_ssl_trustStore=
    javax_net_ssl_trustStorePassword=
    #
    # DB UserId used for the Ranger schema
    #
    db_name=$RANGER_DB
    #TODO: create a proper DB user. Also, secrets.
    db_user=$MYSQL_USER
    db_password=$MYSQL_PASSWORD

    # change password. Password for below mentioned users can be changed only once using this property.
    #PLEASE NOTE :: Password should be minimum 8 characters with min one alphabet and one numeric.
    rangerAdmin_password=$RANGER_PASSWORD
    rangerTagsync_password=$RANGER_PASSWORD
    rangerUsersync_password=$RANGER_PASSWORD
    keyadmin_password=$RANGER_PASSWORD


    #Source for Audit Store. Currently solr and elasticsearch are supported.
    # * audit_store is solr
    audit_store=

    # * audit_solr_url Elasticsearch Host(s). E.g. 127.0.0.1
    audit_elasticsearch_urls=
    audit_elasticsearch_port=
    audit_elasticsearch_protocol=
    audit_elasticsearch_user=
    audit_elasticsearch_password=
    audit_elasticsearch_index=
    audit_elasticsearch_bootstrap_enabled=true


    # * audit_solr_url URL to Solr. E.g. http://<solr_host>:6083/solr/ranger_audits
    audit_solr_urls=
    audit_solr_user=
    audit_solr_password=
    audit_solr_zookeepers=

    audit_solr_collection_name=ranger_audits
    #solr Properties for cloud mode
    audit_solr_config_name=ranger_audits
    audit_solr_no_shards=1
    audit_solr_no_replica=1
    audit_solr_max_shards_per_node=1
    audit_solr_acl_user_list_sasl=solr,infra-solr
    audit_solr_bootstrap_enabled=true

    #------------------------- DB CONFIG - END ----------------------------------

    #
    # ------- PolicyManager CONFIG ----------------
    #

    policymgr_external_url=http://localhost:6080
    policymgr_http_enabled=true
    policymgr_https_keystore_file=
    policymgr_https_keystore_keyalias=rangeradmin
    policymgr_https_keystore_password=

    #Add Supported Components list below separated by semi-colon, default value is empty string to support all components
    #Example :  policymgr_supportedcomponents=hive,hbase,hdfs
    policymgr_supportedcomponents=

    #
    # ------- PolicyManager CONFIG - END ---------------
    #


    #
    # ------- UNIX User CONFIG ----------------
    #
    unix_user=ranger
    unix_user_pwd=ranger
    unix_group=ranger

    #
    # ------- UNIX User CONFIG  - END ----------------
    #
    #

    #
    # UNIX authentication service for Policy Manager
    #
    # PolicyManager can authenticate using UNIX username/password
    # The UNIX server specified here as authServiceHostName needs to be installed with ranger-unix-ugsync package.
    # Once the service is installed on authServiceHostName, the UNIX username/password from the host <authServiceHostName> can be used to login into policy manager
    #
    # ** The installation of ranger-unix-ugsync package can be installed after the policymanager installation is finished.
    #
    #LDAP|ACTIVE_DIRECTORY|UNIX|NONE
    authentication_method=NONE
    remoteLoginEnabled=true
    authServiceHostName=localhost
    authServicePort=5151
    ranger_unixauth_keystore=keystore.jks
    ranger_unixauth_keystore_password=password
    ranger_unixauth_truststore=cacerts
    ranger_unixauth_truststore_password=changeit

    ####LDAP settings - Required only if have selected LDAP authentication ####
    #
    # Sample Settings
    #
    #xa_ldap_url=ldap://127.0.0.1:389
    #xa_ldap_userDNpattern=uid={0},ou=users,dc=xasecure,dc=net
    #xa_ldap_groupSearchBase=ou=groups,dc=xasecure,dc=net
    #xa_ldap_groupSearchFilter=(member=uid={0},ou=users,dc=xasecure,dc=net)
    #xa_ldap_groupRoleAttribute=cn
    #xa_ldap_base_dn=dc=xasecure,dc=net
    #xa_ldap_bind_dn=cn=admin,ou=users,dc=xasecure,dc=net
    #xa_ldap_bind_password=
    #xa_ldap_referral=follow|ignore
    #xa_ldap_userSearchFilter=(uid={0})

    xa_ldap_url=
    xa_ldap_userDNpattern=
    xa_ldap_groupSearchBase=
    xa_ldap_groupSearchFilter=
    xa_ldap_groupRoleAttribute=
    xa_ldap_base_dn=
    xa_ldap_bind_dn=
    xa_ldap_bind_password=
    xa_ldap_referral=
    xa_ldap_userSearchFilter=
    ####ACTIVE_DIRECTORY settings - Required only if have selected AD authentication ####
    #
    # Sample Settings
    #
    #xa_ldap_ad_domain=xasecure.net
    #xa_ldap_ad_url=ldap://127.0.0.1:389
    #xa_ldap_ad_base_dn=dc=xasecure,dc=net
    #xa_ldap_ad_bind_dn=cn=administrator,ou=users,dc=xasecure,dc=net
    #xa_ldap_ad_bind_password=
    #xa_ldap_ad_referral=follow|ignore
    #xa_ldap_ad_userSearchFilter=(sAMAccountName={0})

    xa_ldap_ad_domain=
    xa_ldap_ad_url=
    xa_ldap_ad_base_dn=
    xa_ldap_ad_bind_dn=
    xa_ldap_ad_bind_password=
    xa_ldap_ad_referral=
    xa_ldap_ad_userSearchFilter=

    #------------ Kerberos Config -----------------
    spnego_principal=
    spnego_keytab=
    token_valid=30
    cookie_domain=
    cookie_path=/
    admin_principal=
    admin_keytab=
    lookup_principal=
    lookup_keytab=
    hadoop_conf=/etc/hadoop/conf
    #
    #-------- SSO CONFIG - Start ------------------
    #
    sso_enabled=false
    sso_providerurl=https://127.0.0.1:8443/gateway/knoxsso/api/v1/websso
    sso_publickey=

    #
    #-------- SSO CONFIG - END ------------------

    # Custom log directory path
    RANGER_ADMIN_LOG_DIR=$PWD

    # PID file path
    RANGER_PID_DIR_PATH=/var/run/ranger

    # #################  DO NOT MODIFY ANY VARIABLES BELOW #########################
    #
    # --- These deployment variables are not to be modified unless you understand the full impact of the changes
    #
    ################################################################################
    XAPOLICYMGR_DIR=$PWD
    app_home=$PWD/ews/webapp
    TMPFILE=$PWD/.fi_tmp
    LOGFILE=$PWD/logfile
    LOGFILES="$LOGFILE"

    JAVA_BIN='java'
    JAVA_VERSION_REQUIRED='1.8'
    JAVA_ORACLE='Java(TM) SE Runtime Environment'

    ranger_admin_max_heap_size=1g
    #retry DB and Java patches after the given time in seconds.
    PATCH_RETRY_INTERVAL=120
    STALE_PATCH_ENTRY_HOLD_TIME=10

    #mysql_create_user_file=${PWD}/db/mysql/create_dev_user.sql
    mysql_core_file=db/mysql/optimized/current/ranger_core_db_mysql.sql
    mysql_audit_file=db/mysql/xa_audit_db.sql
    #mysql_asset_file=${PWD}/db/mysql/reset_asset.sql

    #oracle_create_user_file=${PWD}/db/oracle/create_dev_user_oracle.sql
    oracle_core_file=db/oracle/optimized/current/ranger_core_db_oracle.sql
    oracle_audit_file=db/oracle/xa_audit_db_oracle.sql
    #oracle_asset_file=${PWD}/db/oracle/reset_asset_oracle.sql
    #
    postgres_core_file=db/postgres/optimized/current/ranger_core_db_postgres.sql
    postgres_audit_file=db/postgres/xa_audit_db_postgres.sql
    #
    sqlserver_core_file=db/sqlserver/optimized/current/ranger_core_db_sqlserver.sql
    sqlserver_audit_file=db/sqlserver/xa_audit_db_sqlserver.sql
    #
    sqlanywhere_core_file=db/sqlanywhere/optimized/current/ranger_core_db_sqlanywhere.sql
    sqlanywhere_audit_file=db/sqlanywhere/xa_audit_db_sqlanywhere.sql
    cred_keystore_filename=$app_home/WEB-INF/classes/conf/.jceks/rangeradmin.jceks
  app.ini : |
    APP_NAME = Gitea
    RUN_USER = git
    RUN_MODE = prod

    [repository]
    FORCE_PRIVATE = false
    DISABLE_HTTP_GIT = false

    [repository.upload]
    ENABLED = true
    ALLOWED_TYPES =
    FILE_MAX_SIZE = 3
    MAX_FILES = 5

    [server]
    PROTOCOL = http
    HTTP_PORT = 3000
    SSH_dOMAIN = gitea.local
    DOMAIN = gitea.local
    ROOT_URL = https://gitea.local/
    DISABLE_SSH = false
    LANDING_PAGE = home
    SSH_PORT = 22
    REDIRECT_OTHER_PORT = false

    [service]
    ENABLE_CAPTCHA = true
    ACTIVE_CODE_LIVE_MINUTES = 180
    RESET_PASSWD_CODE_LIVE_MINUTES = 180
    REGISTER_EMAIL_CONFIRM = false
    DISABLE_REGISTRATION = false
    REQUIRE_SIGNIN_VIEW = false
    ENABLE_NOTIFY_MAIL = false
    ENABLE_REVERSE_PROXY_AUTHENTICATION = false
    ENABLE_REVERSE_PROXY_AUTO_REGISTRATION = false

    [database]
    DB_TYPE = postgres
    HOST = postgres-gitea
    PORT = 5432
    NAME = gitea
    USER = root
    PASSWD = eagerLamprey
    SSL_MODE = disable

    [admin]
    DISABLE_REGULAR_ORG_CREATION = false

    [security]
    INSTALL_LOCK = true
    SECRET_KEY = "Z2l0ZWE="

    [ui]
    EXPLORE_PAGING_NUM = 20
    ISSUE_PAGING_NUM = 10
    FEED_MAX_COMMIT_NUM = 5

    [cache]
    ADAPTER = memory
    INTERVAL = 60
    HOST =

    [webhook]
    QUEUE_LENGTH = 1000
    DELIVER_TIMEOUT = 5
    SKIP_TLS_VERIFY = true
    PAGING_NUM = 10

    [log]
    MODE = console
    LEVEL = Trace

    [other]
    SHOW_FOOTER_BRANDING = false
    SHOW_FOOTER_VERSION = true
---
# Alluxio server deployment. This is like the virtualized File Allocation Table
# that keeps track of where stuff is mounted as far as distributed storage is
# concerned. Presto interacts with this as it would with HDFS -- Alluxio
# provides a unified interface to talk to a whole bunch of different storage
# solutions.
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: alluxio-server
  name: alluxio-server
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: alluxio-server
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: alluxio-server
    spec:
      volumes:
        - name: alluxio-site
          configMap:
            name: properties
        - name: pre-install
          emptyDir: {}
      initContainers:
      - name: copy-ro-scripts
        image: busybox
        command: ['sh', '-c', 'mkdir -p /opt/alluxio-2.4.0/conf && cp /conf/* /opt/alluxio-2.4.0/conf/']
        volumeMounts:
          - name: alluxio-site
            mountPath: /conf
          - name: pre-install
            mountPath: /opt/alluxio-2.4.0/conf
      containers:
      - name: alluxio-server
        args: ["master"]
        # args:
        # - master
        # command: [ "/bin/bash", "-c", "--" ]
        # args: [ "while true; do sleep 30; done;" ]
        env:
        - name: ALLUXIO_MASTER_ADDRESS
          value: alluxio-server
        - name: ALLUXIO_MASTER_HOSTNAME
          value: alluxio-server
        image: scienz/aorist-test-alluxio
        imagePullPolicy: "Always"
        ports:
        - containerPort: 19200
        - containerPort: 19998
        - containerPort: 19999
        - containerPort: 20000
        - containerPort: 20001
        - containerPort: 30000
        resources: {}
        volumeMounts:
        - name: pre-install
          mountPath: /opt/alluxio/conf
      restartPolicy: Always
      serviceAccountName: ""
---
# Alluxio server service, exposes ports needed by Presto and the Alluxio
# worker.
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: alluxio-server
  name: alluxio-server
  namespace: aorist
spec:
  ports:
  - name: "19998"
    port: 19998
    targetPort: 19998
  - name: "19999"
    port: 19999
    targetPort: 19999
  - name: "20000"
    port: 20000
    targetPort: 20000
  - name: "20001"
    port: 20001
    targetPort: 20001
  - name: "20002"
    port: 20002
    targetPort: 20002
  - name: "20003"
    port: 20003
    targetPort: 20003
  - name: "30000"
    port: 30000
    targetPort: 30000
  - name: "19200"
    port: 19200
    targetPort: 19200
  selector:
    io.kompose.service: alluxio-server
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: presto-coordinator
  name: presto-coordinator
  namespace: aorist
spec:
  serviceName: "presto-coordinator"
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: presto-coordinator
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: presto-coordinator
    spec:
      volumes:
        - name: alluxio-site
          configMap:
            name: properties
        - name: pre-install
          emptyDir: {}
        - name: mem
          emptyDir:
            medium: "Memory"
            sizeLimit: 1G
      terminationGracePeriodSeconds: 10
      initContainers:
      - name: copy-ro-scripts
        image: busybox
        command:
          - sh
          - -c
          - |
            cp /conf/* /opt/alluxio/conf/ &&
            mkdir /opt/alluxio/conf/catalog &&
            cp /conf/hive.properties /opt/alluxio/conf/catalog &&
            cp /conf/core-site.xml /opt/alluxio/conf/catalog &&
            echo 'connector.name=tpch' > /opt/alluxio/conf/catalog/tpch.properties
        volumeMounts:
          - name: alluxio-site
            mountPath: /conf
          - name: pre-install
            mountPath: /opt/alluxio/conf
      containers:
        #- # command: ["bin/launcher", "run", "--verbose"]
        # - command: [ "/bin/bash", "-c", "--" ]
        #  args: [ "while true; do sleep 30; done;" ]
      - env:
        - name: ALLUXIO_MASTER_ADDRESS
          value: alluxio-server
        - name: ALLUXIO_MASTER_HOSTNAME
          value: localhost
        - name: HIVE_METASTORE_HOST
          value: metastore
        - name: HIVE_METASTORE_PORT
          value: "9083"
        - name: ALLUXIO_WORKER_HOSTNAME
          value: $HOSTNAME.presto-coordinator.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          value: $HOSTNAME.presto-coordinator.aorist.svc.cluster.local
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: scienz/aorist-test-presto:latest
        imagePullPolicy: "Always"
        name: presto-coordinator
        ports:
        - containerPort: 8080
        resources: {}
        volumeMounts:
          - name: pre-install
            mountPath: /presto-server-344/etc
      restartPolicy: Always
      serviceAccountName: ""
---
# Presto coordinator service, exposes port 30876 on any of the GKE machines as
# the Presto port.
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    alluxio: worker
  name: alluxio-worker-headless
  namespace: aorist
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: "29998"
    port: 29998
    targetPort: 29998
  - name: "29999"
    port: 29999
    targetPort: 29999
  - name: "job-worker-port"
    port: 30000
    targetPort: 30000
  - name: "job-worker-rpc-port"
    port: 30001
    targetPort: 30001
  - name: "job-worker-data-port"
    port: 30002
    targetPort: 30002
  - name: "job-worker-web-port"
    port: 30003
    targetPort: 30003
  selector:
    alluxio: worker
status:
  loadBalancer: {}
---
# Presto coordinator service, exposes port 30876 on any of the GKE machines as
# the Presto port.
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: presto-coordinator
  name: presto-coordinator
  namespace: aorist
spec:
  type: NodePort
  ports:
  - name: "8080"
    port: 8080
    targetPort: 8080
    nodePort: 30876
  - name: "29998"
    port: 29998
    targetPort: 29998
  - name: "29999"
    port: 29999
    targetPort: 29999
  selector:
    io.kompose.service: presto-coordinator
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: ranger-admin
  name: ranger-admin
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: ranger-admin
  strategy: {}
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: ranger-admin
    spec:
      containers:
      - name: ranger-admin
        image: scienz/aorist-test-ranger-admin:latest
        env:
          - name: MYSQL_HOST
            value: mysql-ranger
          - name: MYSQL_USER
            value: root
          - name: MYSQL_PASSWORD
            value: "eagerLamprey"
          - name: MYSQL_PORT
            value: "3306"
          - name: RANGER_DB
            value: ranger
          - name: RANGER_PASSWORD
            value: G0powerRangers
        imagePullPolicy: "Always"
        ports:
        - containerPort: 6080
        resources: {}
        volumeMounts:
          - mountPath: /conf/ranger-admin-install.properties.template
            name: properties-vol
            subPath: ranger-admin-install.properties.template
            readOnly: false
      volumes:
        - name: properties-vol
          configMap:
            defaultMode: 420
            name: properties
      restartPolicy: Always
      serviceAccountName: ""
status: {}
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: ranger-admin
  name: ranger-admin
  namespace: aorist
spec:
  type: NodePort
  ports:
  - name: "6080"
    port: 6080
    targetPort: 6080
    nodePort: 30800
  selector:
    io.kompose.service: ranger-admin
status:
  loadBalancer: {}
---
# Presto worker deployment. Features a Presto server (set up as a
# coordinator), and an Alluxio worker. Note that Alluxio workers always need to
# be tightly coupled with the Presto server.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: presto-worker
  name: presto-worker
  namespace: aorist
spec:
  serviceName: "presto-worker"
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: presto-worker
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert
        kompose.version: 1.21.0 (992df58d8)
      creationTimestamp: null
      labels:
        io.kompose.service: presto-worker
        alluxio: worker
    spec:
      volumes:
        - name: alluxio-site
          configMap:
            name: properties
        - name: pre-install
          emptyDir: {}
        - name: mem
          emptyDir:
            medium: "Memory"
            sizeLimit: 1Gi
      terminationGracePeriodSeconds: 10
      initContainers:
      - name: copy-ro-scripts
        image: busybox
        command:
          - sh
          - -c
          - |
            cp /conf/* /opt/alluxio/conf/ &&
            mkdir /opt/alluxio/conf/catalog &&
            cp /conf/hive.properties /opt/alluxio/conf/catalog &&
            cp /conf/core-site.xml /opt/alluxio/conf/catalog &&
            echo 'connector.name=tpch' > /opt/alluxio/conf/catalog/tpch.properties
        volumeMounts:
          - name: alluxio-site
            mountPath: /conf
          - name: pre-install
            mountPath: /opt/alluxio/conf
      containers:
        #- # command: ["bin/launcher", "run", "--verbose"]
        # - command: [ "/bin/bash", "-c", "--" ]
        #  args: [ "while true; do sleep 30; done;" ]
      - env:
        - name: ALLUXIO_MASTER_ADDRESS
          value: alluxio-server
        - name: ALLUXIO_MASTER_HOSTNAME
          value: localhost
        - name: HIVE_METASTORE_HOST
          value: metastore
        - name: HIVE_METASTORE_PORT
          value: "9083"
        - name: ALLUXIO_WORKER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: scienz/aorist-test-presto:latest
        imagePullPolicy: "Always"
        name: presto-worker
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "2000m"
            memory: "6Gi"
        volumeMounts:
          - name: pre-install
            mountPath: /presto-server-344/etc
          - mountPath: /dev/shm
            name: mem
      - args:
        - worker-only
        - --no-format
        env:
        - name: ALLUXIO_MASTER_ADDRESS
          value: alluxio-server
        - name: ALLUXIO_MASTER_HOSTNAME
          value: alluxio-server
        - name: ALLUXIO_WORKER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_JAVA_OPTS
          value: " -Dalluxio.worker.hostname=$(ALLUXIO_WORKER_HOSTNAME) "
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: scienz/aorist-test-alluxio:latest
        imagePullPolicy: "Always"
        name: alluxio-worker
        ports:
        - containerPort: 29998
        - containerPort: 29999
        resources: {}
        volumeMounts:
          - name: pre-install
            mountPath: /opt/alluxio/conf
          - mountPath: /dev/shm
            name: mem
      - args:
        - job-worker
        - --no-format
        env:
        - name: ALLUXIO_MASTER_ADDRESS
          value: alluxio-server
        - name: ALLUXIO_MASTER_HOSTNAME
          value: alluxio-server
        - name: ALLUXIO_WORKER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_CONTAINER_HOSTNAME
          value: $HOSTNAME.presto-worker.aorist.svc.cluster.local
        - name: ALLUXIO_WORKER_JAVA_OPTS
          value: " -Dalluxio.worker.hostname=$(ALLUXIO_WORKER_HOSTNAME) "
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: scienz/aorist-test-alluxio:latest
        imagePullPolicy: "Always"
        name: alluxio-job-worker
        ports:
        - containerPort: 30000
        - containerPort: 30001
        - containerPort: 30002
        - containerPort: 30003
        resources: {}
        volumeMounts:
          - name: pre-install
            mountPath: /opt/alluxio/conf
          - mountPath: /dev/shm
            name: mem
      restartPolicy: Always
      serviceAccountName: ""
---
# Presto coordinator service, exposes port 30876 on any of the GKE machines as
# the Presto port.
apiVersion: v1
kind: Service
metadata:
  annotations:
    kompose.cmd: kompose convert
    kompose.version: 1.21.0 (992df58d8)
  creationTimestamp: null
  labels:
    io.kompose.service: presto-coordinator
  name: presto-coordinator-headless
  namespace: aorist
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: "8080"
    port: 8080
    targetPort: 8080
  selector:
    io.kompose.service: presto-coordinator
status:
  loadBalancer: {}
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: postgres-pv-claim
  namespace: aorist
  labels:
    app: postgres
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:10.4
          imagePullPolicy: "IfNotPresent"
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: hasura
            - name: POSTGRES_USER
              value: root
            - name: POSTGRES_PASSWORD
              value: eagerLamprey
            - name: PGDATA
              value: /var/lib/postgresql/data/d
          volumeMounts:
            - mountPath: /var/lib/postgresql/data
              name: postgredb
      volumes:
        - name: postgredb
          persistentVolumeClaim:
            claimName: postgres-pv-claim
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: aorist
  labels:
    app: postgres
spec:
  type: NodePort
  ports:
   - port: 5432
  selector:
   app: postgres
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    service: hasura
  name: hasura
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      service: hasura
  strategy: {}
  template:
    metadata:
      labels:
        service: hasura
    spec:
      containers:
        - args:
            - graphql-engine
            - serve
          env:
            - name: HASURA_GRAPHQL_DATABASE_URL
              value: postgres://root:eagerLamprey@postgres:5432/hasura
            - name: HASURA_GRAPHQL_ENABLE_CONSOLE
              value: "true"
            - name: HASURA_GRAPHQL_QUERY_PLAN_CACHE_SIZE
              value: "100"
            - name: HASURA_GRAPHQL_SERVER_PORT
              value: "3000"
          image: hasura/graphql-engine:v1.3.2
          imagePullPolicy: "IfNotPresent"
          name: hasura
          ports:
            - containerPort: 3000
            - containerPort: 8080
              protocol: TCP
          resources: {}
      restartPolicy: Always
      serviceAccountName: "prefect"
      volumes: null
status: {}
---
apiVersion: v1
kind: Service
metadata:
  name: hasura-node-ip
  namespace: aorist
  labels:
    app: hasura
spec:
  type: NodePort
  ports:
    - name: hasura
      port: 8080
      targetPort: hasura
      nodePort: 30801
  selector:
    app: hasura
---

apiVersion: v1
kind: Service
metadata:
  labels:
    service: hasura
  name: hasura
  namespace: aorist
spec:
  ports:
  - name: "3000"
    port: 3000
    targetPort: 3000
  selector:
    service: hasura
status:
  loadBalancer: {}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prefect
  namespace: aorist
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: prefect-role
  namespace: aorist
  labels:
    app.kubernetes.io/component: agent
rules:
  - apiGroups:
      - batch
      - extensions
    resources:
      - jobs
    verbs:
      - "*"
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: prefect-role-binding
  namespace: aorist
  labels:
    app.kubernetes.io/component: agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: prefect-role
subjects:
  - kind: ServiceAccount
    name: prefect
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    service: graphql
  name: graphql
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      service: graphql
  strategy: {}
  template:
    metadata:
      labels:
        service: graphql
    spec:
      containers:
        - args:
            - bash
            - -c
            - "prefect-server database upgrade -y && python src/prefect_server/services/graphql/server.py"
          env:
            - name: PREFECT_SERVER__DATABASE__CONNECTION_URL
              value: postgresql://root:eagerLamprey@postgres:5432/hasura
            - name: PREFECT_SERVER__HASURA__ADMIN_SECRET
              value: hasura-secret-admin-secret
            - name: PREFECT_SERVER__HASURA__HOST
              value: hasura
            - name: PREFECT_SERVER__SERVICES__HOST
              value: 0.0.0.0
          image: prefecthq/server:latest
          imagePullPolicy: "Always"
          name: graphql
          ports:
            - containerPort: 4201
          resources: {}
      restartPolicy: Always
      serviceAccountName: "prefect"
      volumes: null
status: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    service: graphql
  name: graphql
  namespace: aorist
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: "4200"
    port: 4200
    targetPort: 4200
  - name: "4201"
    port: 4201
    targetPort: 4201
  selector:
    service: graphql
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    service: apollo
  name: apollo
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      service: apollo
  strategy: {}
  template:
    metadata:
      labels:
        service: apollo
    spec:
      containers:
        - args:
            - npm
            - run
            - serve
          env:
            - name: HASURA_API_URL
              value: http://hasura:3000/v1alpha1/graphql
            - name: PREFECT_API_HEALTH_URL
              value: http://graphql:4201/health
            - name: PREFECT_API_URL
              value: 'http://graphql:4201/graphql/'
            - name: POD_NAMESPACE
              value: default
            - name: APOLLO_API_BIND_ADDRESS
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          image: prefecthq/apollo:latest
          imagePullPolicy: "IfNotPresent"
          name: apollo
          ports:
            - name: graphql
              containerPort: 4200
              protocol: TCP
              # livenessProbe:
              #   tcpSocket:
              #     port: 4200
              #   initialDelaySeconds: 5
              #   periodSeconds: 10
              #   timeoutSeconds: 1
              #   successThreshold: 1
              #   failureThreshold: 3
              # readinessProbe:
              #   tcpSocket:
              #     port: 4200
              #   initialDelaySeconds: 5
              #   periodSeconds: 10
              #   timeoutSeconds: 1
              #   successThreshold: 1
              #   failureThreshold: 3
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
      restartPolicy: Always
      serviceAccountName: "prefect"
      volumes: null
status: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    service: apollo
  name: apollo
  namespace: aorist
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 4200
      targetPort: graphql
      protocol: TCP
      name: graphql
  selector:
    service: apollo
status:
  loadBalancer: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    service: apollo
  name: apollo-node
  namespace: aorist
spec:
  type: NodePort
  ports:
  - name: "4200"
    port: 4200
    targetPort: 4200
    nodePort: 30802
  - name: "4201"
    port: 4201
    targetPort: 4201
    nodePort: 30803
  selector:
    service: apollo
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    service: ui
  name: ui
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      service: ui
  strategy: {}
  template:
    metadata:
      labels:
        service: ui
    spec:
      initContainers:
        - name: init-apollo
          image: "busybox:1.28"
          command:
            - sh
            - "-c"
            - "until nslookup apollo.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for apollo; sleep 2; done"
      containers:
        - name: ui
          ## TODO: pin this at 0.10.5 or later when available
          image: prefecthq/ui:core-0.13.12
          imagePullPolicy: "IfNotPresent"
          args:
            - /intercept.sh
          env:
            - name: PREFECT_SERVER__APOLLO_URL
              value: "http://localhost:30802/graphql"
          ports:
            - containerPort: 8080
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 32Mi
      restartPolicy: Always
      serviceAccountName: "prefect"
status: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    service: ui
  name: ui
  namespace: aorist
spec:
  type: NodePort
  ports:
  - name: "8080"
    port: 8080
    targetPort: 8080
    nodePort: 30804
  selector:
    service: ui
status:
  loadBalancer: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prefect-agent
  namespace: aorist
  labels:
    app.kubernetes.io/component: agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: agent
  template:
    metadata:
      labels:
        app.kubernetes.io/component: agent
    spec:
      serviceAccountName: prefect
      initContainers:
        - name: init-apollo
          image: "busybox:1.28"
          command:
            - sh
            - "-c"
            - "until nslookup apollo.aorist.svc.cluster.local; do echo waiting for apollo; sleep 2; done"
          # TODO: create default tenant prefect server create-tenant --name $POD_NAME --slug $POD_NAME
      containers:
        - name: agent
          image: "prefecthq/prefect:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - "-c"
          args:
            - "prefect agent start --name 'Local Agent'"
          volumeMounts:
            - name: gcloud
              mountPath: /gcloud
          env:
            - name: GOOGLE_APPLICATION_CREDENTIALS
              value: gcloud/social_norms.json
            - name: PREFECT__BACKEND
              value: server
            - name: NAMESPACE
              value: default
            - name: PREFECT__SERVER__HOST
              value: http://apollo
            - name: PREFECT__SERVER__PORT
              value: '4200'
            - name: IMAGE_PULL_SECRETS
              value: ''
            - name: PREFECT__CLOUD__AGENT__LABELS
              value: "[]"
            - name: JOB_MEM_REQUEST
              value: 128Mi
            - name: JOB_MEM_LIMIT
              value: 1Gi
            - name: JOB_CPU_REQUEST
              value: 50m
            - name: JOB_CPU_LIMIT
              value: 500m
            - name: PREFECT__CLOUD__AGENT__AUTH_TOKEN
              value: ""
            - name: PREFECT__LOGGING__LEVEL
              value: DEBUG
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: PREFECT__CLOUD__AGENT__AGENT_ADDRESS
              value: http://$POD_IP:8080
              # livenessProbe:
              #   exec:
              #     command:
              #       - python
              #       - -c
              #       - from prefect.agent.kubernetes.agent import check_heartbeat; check_heartbeat()
              #   initialDelaySeconds: 40
              #   periodSeconds: 40
              #   timeoutSeconds: 1
              #   successThreshold: 1
              #   failureThreshold: 2
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
        - name: kube-agent
          image: "prefecthq/prefect:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - "-c"
          args:
            - "prefect agent start kubernetes --name 'Kube Agent'"
          volumeMounts:
            - name: gcloud
              mountPath: /gcloud
          env:
            - name: GOOGLE_APPLICATION_CREDENTIALS
              value: gcloud/social_norms.json
            - name: PREFECT__BACKEND
              value: server
            - name: NAMESPACE
              value: default
            - name: PREFECT__SERVER__HOST
              value: http://apollo
            - name: PREFECT__SERVER__PORT
              value: '4200'
            - name: IMAGE_PULL_SECRETS
              value: ''
            - name: PREFECT__CLOUD__AGENT__LABELS
              value: "[]"
            - name: JOB_MEM_REQUEST
              value: 128Mi
            - name: JOB_MEM_LIMIT
              value: 1Gi
            - name: JOB_CPU_REQUEST
              value: 50m
            - name: JOB_CPU_LIMIT
              value: 500m
            - name: PREFECT__CLOUD__AGENT__AUTH_TOKEN
              value: ""
            - name: PREFECT__LOGGING__LEVEL
              value: DEBUG
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: PREFECT__CLOUD__AGENT__AGENT_ADDRESS
              value: http://$POD_IP:8080
              # livenessProbe:
              #   exec:
              #     command:
              #       - python
              #       - -c
              #       - from prefect.agent.kubernetes.agent import check_heartbeat; check_heartbeat()
              #   initialDelaySeconds: 40
              #   periodSeconds: 40
              #   timeoutSeconds: 1
              #   successThreshold: 1
              #   failureThreshold: 2
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
        - name: resource-manager
          image: "prefecthq/prefect:latest"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -c
          args:
            - python -c 'from prefect.agent.kubernetes import ResourceManager; ResourceManager().start()'
          env:
            - name: PREFECT__BACKEND
              value: server
            - name: NAMESPACE
              value: default
            - name: PREFECT__CLOUD__API
              value: "http://apollo.aorist.svc.cluster.local:4200"
            - name: PREFECT__CLOUD__AGENT__RESOURCE_MANAGER__LOOP_INTERVAL
              value: "60"
            - name: PREFECT__CLOUD__AGENT__AUTH_TOKEN
              value: ""
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
      volumes:
      - name: gcloud
        secret:
          secretName: gcloud

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    service: scheduler
  name: scheduler
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      service: scheduler
  strategy: {}
  template:
    metadata:
      labels:
        service: scheduler
    spec:
      containers:
        - command: [ "/bin/bash", "-c", "--" ]
          args:
          - "python src/prefect_server/services/towel/"
          env:
            - name: PREFECT_SERVER__DATABASE__CONNECTION_URL
              value: postgresql://root:eagerLamprey@postgres:5432/hasura
            - name: PREFECT_SERVER__HASURA__ADMIN_SECRET
              value: hasura-secret-admin-secret
            - name: PREFECT_SERVER__HASURA__HOST
              value: hasura
            - name: PREFECT_SERVER__SERVICES__HOST
              value: 0.0.0.0
          image: "prefecthq/server:latest"
          imagePullPolicy: "IfNotPresent"
          name: scheduler
          resources: {}
      restartPolicy: Always
      serviceAccountName: "prefect"
      volumes: null
status: {}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitea
  namespace: aorist
  labels:
    app: gitea
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitea
  template:
    metadata:
      labels:
        app: gitea
    spec:
      initContainers:
      - name: init-disk
        image: busybox:latest
        imagePullPolicy: IfNotPresent
        command:
          - sh
          - -c
          - |
            /bin/chown 1000:1000 /data &&
            cp /conf/app.ini /data/gitea/conf &&
            /bin/chown 1000:1000 /data/gitea/conf/app.ini
        volumeMounts:
        - name: git-data
          mountPath: "/data"
        - name: gitea-config
          mountPath: /conf
        - name: pre-install
          mountPath: /data/gitea/conf
      containers:
      - name: gitea
        image: scienz/aorist-test-gitea:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 80
        - containerPort: 3000
        - containerPort: 22
        volumeMounts:
        - mountPath: /data
          name: git-data
        - mountPath: /data/gitea/conf
          name: pre-install
          readOnly: false
        - name: aorist-master-key
          mountPath: /home/git/.ssh
        - name: admin-token
          mountPath: /admin-token
      - name: gitea-agent
        image: scienz/aorist-test-gitea-agent:latest
        imagePullPolicy: Always
        volumeMounts:
        - name: aorist-master-key
          mountPath: /aorist-master-key
        - name: admin-token
          mountPath: /admin-token
      volumes:
      - name: git-data
        persistentVolumeClaim:
          claimName: git-data-pvc
      - name: gitea-config
        configMap:
          name: properties
      - name: pre-install
        emptyDir: {}
      - name: aorist-master-key
        secret:
          secretName: aorist-master-key
          defaultMode: 0600
      - name: admin-token
        emptyDir: {}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: git-data-pvc
  namespace: aorist
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1G
---
apiVersion: v1
kind: Service
metadata:
  name: gitea
  namespace: aorist
  labels:
    app: gitea
spec:
  type: NodePort
  ports:
    - name: gitea
      port: 3000
      targetPort: 3000
      nodePort: 30807
    - name: "80"
      port: 80
      targetPort: 80
      nodePort: 30808
  selector:
    app: gitea
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: postgres-gitea-pv-claim
  namespace: aorist
  labels:
    app: postgres-gitea
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-gitea
  namespace: aorist
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-gitea
  template:
    metadata:
      labels:
        app: postgres-gitea
    spec:
      containers:
        - name: postgres
          image: postgres:10.4
          imagePullPolicy: "IfNotPresent"
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: gitea
            - name: POSTGRES_USER
              value: root
            - name: POSTGRES_PASSWORD
              value: eagerLamprey
            - name: PGDATA
              value: /var/lib/postgresql/data/d
          volumeMounts:
            - mountPath: /var/lib/postgresql/data
              name: postgredb
      volumes:
        - name: postgredb
          persistentVolumeClaim:
            claimName: postgres-gitea-pv-claim
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-gitea
  namespace: aorist
  labels:
    app: postgres-gitea
spec:
  type: NodePort
  ports:
   - port: 5432
  selector:
   app: postgres-gitea
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: mysql-ranger
  namespace: aorist
spec:
  selector:
    matchLabels:
      app: mysql-ranger
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql-ranger
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: eagerLamprey
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pvc-ranger
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-ranger
  namespace: aorist
spec:
  ports:
  - port: 3306
    targetPort: 3306
  selector:
    app: mysql-ranger
  clusterIP: None
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mysql-pvc-ranger
  namespace: aorist
  labels:
    app: mysql-pvc-ranger
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
