---
type: Program
spec:
  use: UploadDataToAlluxio
  root: AlluxioLocation
  dialect: Python
  preamble: |
    import alluxio
    from alluxio import option, wire
    def upload_to_alluxio(hostname, port, schema, directory, tablename, tmp_dir, source_file):
      directory = '/' + directory + '/' + schema + '/' + tablename
      client = alluxio.Client(hostname, int(port))
      opt = option.CreateDirectory(recursive=True, allow_exists=True, write_type=wire.WRITE_TYPE_MUST_CACHE)
      client.create_directory(directory, opt)
      path = directory + "/data.csv"
      if client.exists(path):
        client.delete(path)
      opt = option.CreateFile(write_type=wire.WRITE_TYPE_MUST_CACHE)
      with client.open(path, 'w', opt) as f:
        with open(tmp_dir + '/' + source_file) as source:
          try:
            f.write(source)
          except:
            pass
      print("Done uploading %s to %s" % (source_file, path))
  call: upload_to_alluxio
  args:
    - type: AncestorArgument
      spec:
        call: universe.endpoints.alluxio.as_ref().unwrap().server.clone()
        attaches: Universe
    - type: AncestorArgument
      spec:
        call: format!("{}", universe.endpoints.alluxio.as_ref().unwrap().apiPort).to_string()
        attaches: Universe
    - type: AncestorArgument
      spec:
        call: data_set.name
        attaches: DataSet
    - type: AncestorArgument
      spec:
        call: universe.endpoints.alluxio.as_ref().unwrap().directory.clone()
        attaches: Universe
    - type: AncestorArgument
      spec:
        call: format!("{}_csv", static_data_table.name).to_string()
        attaches: StaticDataTable
    - type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
    - type: AncestorArgument
      spec:
        call: format!("{}.csv", static_data_table.name).to_string()
        attaches: StaticDataTable
---
type: Program
spec:
  use: UploadDataToMinio
  root: MinioLocation
  dialect: Python
  preamble: |
    from minio import Minio
    def upload_to_minio(hostname, port, access_key, secret_key, bucket, schema, tablename, tmp_dir, source_file):
        client = Minio(
            "%s:%s" % (hostname, port),
            access_key=access_key,
            secret_key=secret_key,
            secure=False,
        )

        assert client.bucket_exists(bucket)
        dest_path = schema + '/' + tablename + '/data.csv'
        source_path = tmp_dir + "/" + source_file
        client.fput_object(bucket, dest_path, source_path)
        print("Successfully uploaded %s to %s" % (source_path, dest_path))
  call: upload_to_minio
  kwargs:
    hostname:
      type: AncestorArgument
      spec:
        call: universe.endpoints.minio.as_ref().unwrap().server
        attaches: Universe
    port:
      type: AncestorArgument
      spec:
        call: universe.endpoints.minio.as_ref().unwrap().port.to_string()
        attaches: Universe
    access_key:
      type: AncestorArgument
      spec:
        call: universe.endpoints.minio.as_ref().unwrap().access_key
        attaches: Universe
    secret_key:
      type: AncestorArgument
      spec:
        call: universe.endpoints.minio.as_ref().unwrap().secret_key
        attaches: Universe
    bucket:
      type: AncestorArgument
      spec:
        call: universe.endpoints.minio.as_ref().unwrap().bucket
        attaches: Universe
    schema:
      type: AncestorArgument
      spec:
        call: data_set.name
        attaches: DataSet
    tablename:
      type: AncestorArgument
      spec:
        call: format!("{}_csv", static_data_table.name).to_string()
        attaches: StaticDataTable
    tmp_dir:
      type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
    source_file:
      type: AncestorArgument
      spec:
        call: format!("{}.csv", static_data_table.name).to_string()
        attaches: StaticDataTable
---
type: Program
spec:
  use: DownloadDataFromRemoteGCSLocation
  root: GCSLocation
  dialect: Python
  preamble: |
    from google.cloud import storage
    import os
    def download_blob_to_file(bucket_name, blob_name, tmp_dir, file_name):
      client = storage.Client.from_service_account_json('/home/bogdan/.gcloud/social_norms.json')
      if not os.path.exists(tmp_dir):
          os.makedirs(tmp_dir)
      bucket = client.bucket(bucket_name)
      blob = bucket.blob(blob_name)
      dest = "%s/%s" % (tmp_dir, file_name)
      blob.download_to_filename(dest)
  call: download_blob_to_file
  args:
    - type: AncestorArgument
      spec:
        call: gcs_location.bucket
        attaches: GCSLocation
    - type: AncestorArgument
      spec:
        call: gcs_location.blob
        attaches: GCSLocation
    - type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
    - type: AncestorArgument
      spec:
        call: format!("{}.downloaded", static_data_table.name).to_string()
        attaches: StaticDataTable
---
type: Program
spec:
  use: DownloadDataFromRemoteWebLocation
  root: WebLocation
  dialect: Bash
  call: mkdir -p {tmp_dir} && curl {address} -o {tmp_dir}/{file_name}
  kwargs:
    address:
      type: AncestorArgument
      spec:
        call: web_location.address
        attaches: WebLocation
        name: address
    tmp_dir:
      type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
        name: tmp_dir
    file_name:
      type: AncestorArgument
      spec:
        call: format!("{}.downloaded", static_data_table.name).to_string()
        attaches: StaticDataTable
        name: file_name
---
type: Program
spec:
  use: DownloadDataFromRemotePushshiftAPILocation
  root: PushshiftAPILocation
  dialect: Python
  preamble: |
      def download_subreddit(subreddit, tmp_dir, file_name):
          
          from pmaw import PushshiftAPI
          import json
          
          import os
          if not os.path.exists(tmp_dir):
              os.makedirs(tmp_dir)

          output_file = '%s/%s' % (tmp_dir, file_name)

          api = PushshiftAPI(
              num_workers=16,
              limit_type='backoff',
              jitter='decorr',
          )
          posts = api.search_submissions(
              subreddit=subreddit,
              limit=10e9,
              after=0
          )
          with open(output_file, 'w') as f:
            for post in posts:
                line = json.dumps(post) + '\\n'
                f.write(line)
  call: download_subreddit
  kwargs:
    subreddit:
      type: AncestorArgument
      spec:
        call: pushshift_api_location.subreddit
        attaches: PushshiftAPILocation
        name: subreddit
    tmp_dir:
      type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
        name: tmp_dir
    file_name:
      type: AncestorArgument
      spec:
        call: format!("{}.downloaded", static_data_table.name).to_string()
        attaches: StaticDataTable
        name: file_name
---
type: Program
spec:
  use: DecompressZip
  root: ZipCompression
  dialect: Bash
  call: unzip {command}
  kwargs:
    command:
      type: MultipleAncestorsArgument
      spec:
        name: command
        call: >
          match &zip_compression.filename {
            Some(ref file) => format!(
                "-p {tmp_dir}/{archive_name}.downloaded {file_name} > {tmp_dir}/{archive_name}.txt",
                tmp_dir=remote_import_storage_setup.tmp_dir,
                archive_name=static_data_table.name,
                file_name=file,
            ).to_string(),
            None => format!(
                "-p {tmp_dir}/{archive_name}.downloaded > {tmp_dir}/{archive_name}.txt",
                tmp_dir=remote_import_storage_setup.tmp_dir,
                archive_name=static_data_table.name,
            )
          }
        attaches:
          - ZipCompression
          - StaticDataTable
          - RemoteImportStorageSetup
---
type: Program
spec:
  use: DecompressGzip
  root: GzipCompression
  dialect: Bash
  call: gunzip {command}
  kwargs:
    command:
      type: MultipleAncestorsArgument
      spec:
        name: command
        call: >
            format!(
                "--suffix=downloaded -c {tmp_dir}/{archive_name}.downloaded > {tmp_dir}/{archive_name}.txt",
                tmp_dir=remote_import_storage_setup.tmp_dir,
                archive_name=static_data_table.name,
            )
        attaches:
          - StaticDataTable
          - RemoteImportStorageSetup
---
type: Program
spec:
  use: RemoveFileHeader
  root: FileHeader
  dialect: Bash
  call: tail -n +{n} {tmp_dir}/{file_name}.txt > {tmp_dir}/{file_name}.no_header && rm {tmp_dir}/{file_name}.txt
  kwargs:
    n:
      type: AncestorArgument
      spec:
        attaches: FileHeader
        call: format!("{}", file_header.get_num_lines() + 1)
    tmp_dir:
      type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
        name: tmp_dir
    file_name:
      type: AncestorArgument
      spec:
        call: static_data_table.name
        attaches: StaticDataTable
        name: file_name
---
type: Program
spec:
  use: HiveDirectoriesCreated
  root: OnPremiseLocation
  dialect: Presto
  call: CREATE SCHEMA IF NOT EXISTS {presto_schema} {location}
  kwargs:
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
    location:
      type: MultipleAncestorsArgument
      spec:
        attaches:
           - HiveTableStorage
           - Universe
           - DataSet
        call: >
          match &hive_table_storage.location {
            HiveLocation::AlluxioLocation(a) => format!(
                "WITH (location='alluxio://{}:{}/{}/{}/{}')",
                universe.endpoints.alluxio.as_ref().unwrap().server,
                universe.endpoints.alluxio.as_ref().unwrap().rpcPort,
                universe.endpoints.alluxio.as_ref().unwrap().directory,
                data_set.name,
                a.path,
            ).to_string(),
            HiveLocation::MinioLocation(_) => format!(
                "WITH (location='s3a://{}/{}/')",
                universe.endpoints.minio.as_ref().unwrap().bucket,
                data_set.name,
            ).to_string(),
          }
---
type: Program
spec:
  use: TableSchemasCreated
  root: HiveTableStorage
  dialect: Presto
  call: |
      \nCREATE TABLE IF NOT EXISTS hive.{presto_schema}.{table_name}
      ({schema}) WITH (format='{data_format}')
  kwargs:
    table_name:
      type: AncestorArgument
      spec:
        call: asset.get_name()
        attaches: Asset
    data_format:
      type: AncestorArgument
      spec:
        call: >
          match &hive_table_storage.encoding {
            Encoding::ORCEncoding(_) => "ORC".to_string(),
            Encoding::CSVEncoding(_) => "CSV".to_string(),
            _ => panic!("Format not supported for hive table")
          }
        attaches: HiveTableStorage
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
    schema:
      type: MultipleAncestorsArgument
      spec:
        call: >
          {
            use crate::template::TDatumTemplate;
            let DataSchema::TabularSchema(ref t) = asset.get_schema();
            let dictionary = Standard::from_embedded(Language::EnglishUS).unwrap();
            let options = Options::new(66).splitter(dictionary);

            let attr_v = data_set
                .datumTemplates
                .iter()
                .filter(|x| {
                    x.get_name().clone() == t.datumTemplateName
                })
                .map(|x| x.get_attributes())
                .next().unwrap();
            let attr_map = attr_v.iter()
                .map(|x|
                    (x.get_name().clone(), (x.get_presto_type(), x.get_comment().clone())))
                .collect::<Vec<_>>();
            let attr_types = attr_map.iter().map(|x| {
                let (name, (presto_type, comment)) = x;
                let comment_str = match comment {
                    Some(c) => {
                        let cleaned_up_comment = c
                          .split("\n")
                          .into_iter()
                          .map(|x| x.to_string().trim().to_string())
                          .collect::<Vec<String>>().join(" ");
                        fill(cleaned_up_comment.trim(), &options)
                          .split("\n").map(|x| x.to_string()).collect::<Vec<String>>()
                    }
                    None => vec!["".to_string()],
                };
                if comment_str.len() == 1 {
                    return format!(
                        "    {} {} COMMENT '{}'",
                        name,
                        presto_type,
                        comment_str.get(0).unwrap().to_string().replace("\'", "`"),
                    ).to_string();
                } else {
                    return format!(
                        "    {} {} \n COMMENT {}\n",
                        name,
                        presto_type,
                        comment_str
                          .into_iter()
                          .map(|x| format!("        '{}'", x.to_string().replace("\'", "`")).to_string())
                          .collect::<Vec<String>>().join("\n")
                    ).to_string();
                }
            }).collect::<Vec<String>>();
            format!("\n{}    \n", attr_types.join(",\n"))
          }
        attaches:
          - Asset
          - DataSet
---
type: Program
spec:
  use: TableSchemasDroppedIfExisting
  root: HiveTableStorage
  dialect: Presto
  call: |
      \nDROP TABLE IF EXISTS hive.{presto_schema}.{table_name}
  kwargs:
    table_name:
      type: AncestorArgument
      spec:
        call: asset.get_name()
        attaches: Asset
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
---
type: Program
spec:
  use: DroppedCSVTable
  root: HiveTableStorage
  dialect: Presto
  call: |
      \nDROP TABLE IF EXISTS hive.{presto_schema}.{table_name}_csv
  kwargs:
    table_name:
      type: AncestorArgument
      spec:
        call: static_data_table.name
        attaches: StaticDataTable
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
---
type: Program
spec:
  use: CSVTableSchemasCreated
  root: HiveTableStorage
  dialect: Presto
  call: \nCREATE TABLE IF NOT EXISTS {presto_schema}.{table_name} \n({schema})\nWITH (format='CSV', external_location='{external_location}')\n
  kwargs:
    external_location:
      type: MultipleAncestorsArgument
      spec:
        attaches:
           - HiveTableStorage
           - Universe
           - StaticDataTable
           - DataSet
        call: >
          match &hive_table_storage.location {
            HiveLocation::AlluxioLocation(_) => format!(
                "alluxio://{}:{}/{}/{}/{}_csv",
                universe.endpoints.alluxio.as_ref().unwrap().server,
                universe.endpoints.alluxio.as_ref().unwrap().rpcPort,
                universe.endpoints.alluxio.as_ref().unwrap().directory,
                data_set.name,
                static_data_table.name,
            ).to_string(),
            HiveLocation::MinioLocation(_) => format!(
                "s3a://{}/{}/{}_csv/",
                universe.endpoints.minio.as_ref().unwrap().bucket,
                data_set.name,
                static_data_table.name,
            ).to_string(),
          }
    table_name:
      type: AncestorArgument
      spec:
        call: format!("{}_csv", static_data_table.name).to_string()
        attaches: StaticDataTable
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
    schema:
      type: MultipleAncestorsArgument
      spec:
        call: > 
          {
              let replicate_schema = || {
                use crate::template::TDatumTemplate;
                let DataSchema::TabularSchema(ref t) = static_data_table.schema;
                let attr_v = data_set
                    .datumTemplates
                    .iter()
                    .filter(|x| {
                        x.get_name().clone() == t.datumTemplateName
                    })
                    .map(|x| x.get_attributes())
                    .next().unwrap();
                let attr_map = attr_v.iter()
                    .map(|x|
                        (x.get_name().clone(), "VARCHAR".to_string()))
                    .collect::<Vec<_>>();
                let attr_types = attr_map.iter().map(|x| {
                    let (name, presto_type) = x;
                    return format!(
                        "    {} {}",
                        name,
                        presto_type,
                    ).to_string();
                }).collect::<Vec<String>>();
                format!("\n{}\n", attr_types.join(",\n"))
              };
              match storage_setup {
                crate::StorageSetup::RemoteImportStorageSetup(r) =>
                  match r.remote.get_encoding() {
                    Some(crate::Encoding::JSONEncoding(_)) => { 
                        "\njson_obj VARCHAR\n".to_string()
                      },
                    _ => replicate_schema(),
                  },
                _ => replicate_schema(),
              }
          }
        attaches:
          - StaticDataTable
          - DataSet
          - StorageSetup
---
type: Program
spec:
  use: ConvertToCSV
  root: RemoteStorage
  dialect: Bash
  call: "{call}"
  kwargs:
    call:
      type: MultipleAncestorsArgument
      spec:
        call: >
          {
            let tmp_dir = &remote_import_storage_setup.tmp_dir;
            let file_name = &static_data_table.name;
            let source = match &remote_storage.encoding.get_header() {
                Some(_) => format!("{}/{}.no_header", tmp_dir, file_name).to_string(),
                None => match &remote_storage.encoding.get_compression() {
                    Some(_) => format!("{}/{}.txt", tmp_dir, file_name),
                    None => format!("{}/{}.downloaded", tmp_dir, file_name),
                }
            };
            match &remote_storage.encoding {
               Encoding::CSVEncoding(_) | Encoding::JSONEncoding(_) => format!(
                  "mv {source} {tmp_dir}/{file_name}.csv",
                  source=source,
                  tmp_dir=tmp_dir,
                  file_name=file_name,
               ).to_string(),
               Encoding::TSVEncoding(_) => format!(
                  "cat {source} | tr '\\\\t' ',' > {tmp_dir}/{file_name}.csv",
                  source=source,
                  tmp_dir=tmp_dir,
                  file_name=file_name,
               ).to_string(),
              _ => panic!("Encoding not supported.")
            }
          }
        attaches:
          - StaticDataTable
          - RemoteImportStorageSetup
          - RemoteStorage
---
type: Program
spec:
  use: ConvertCSVTableToORCTable
  root: HiveTableStorage
  dialect: Presto
  call: >
      INSERT INTO {presto_schema}.{table_name}
      SELECT {columns} 
      FROM {source_table}
  kwargs:
    source_table:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - StaticDataTable
          - DataSet
          - StorageSetup
        call: >
          {
            let source = format!("{}.{}", data_set.name, static_data_table.name).to_string();
              match storage_setup {
                crate::StorageSetup::RemoteImportStorageSetup(r) =>
                  match r.remote.get_encoding() {
                    Some(crate::Encoding::JSONEncoding(_)) => format!(
                        "(SELECT 
                            JSON_PARSE(json_obj) AS json_obj 
                          FROM {})",
                        source
                    ).to_string(),
                    _ => source,
                  },
                  _ => source,
              }
          }
    columns:
      type: MultipleAncestorsArgument
      spec:
        call: >
          {
              use crate::template::TDatumTemplate;
              let DataSchema::TabularSchema(ref t) = static_data_table.schema;
              let attr_v = data_set
                  .datumTemplates
                  .iter()
                  .filter(|x| {
                      x.get_name().clone() == t.datumTemplateName
                  })
                  .map(|x| x.get_attributes())
                  .next().unwrap();
              let attr_map = attr_v.iter()
                  .map(|x|
                      (x.get_name().clone(), x.get_presto_type()))
                  .collect::<Vec<_>>();
              
              let from_json = match storage_setup {
                  crate::StorageSetup::RemoteImportStorageSetup(r) => 
                    match r.remote.get_encoding() {
                      Some(crate::Encoding::JSONEncoding(_)) => true,
                      _ => false,
                    },
                  _ => false,
              };

              let attr_types = attr_map.iter().map(|x| {
                  let (name, presto_type) = x;
                  return match from_json {
                    false => format!(
                        "CAST({name} AS {presto_type}) AS {name}", 
                        name=name, 
                        presto_type=presto_type,
                    ),
                    true => format!(
                        "CAST(json_obj.{name} AS {presto_type}) AS {name})",
                        name=name,
                        presto_type=presto_type,
                    )
                  }.to_string();
              }).collect::<Vec<String>>();
              
              format!("\n{}", attr_types.join(",\n"))
          }
        attaches:
          - StaticDataTable
          - DataSet
          - StorageSetup
---
type: Program
spec:
  use: HistogramsComputed
  root: StaticDataTable
  dialect: Presto
  call: "{query}"
  kwargs:
    query:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - StaticDataTable
          - DataSet
        call: >
          {
            use crate::template::DatumTemplate;
            let DataSchema::TabularSchema(ref t) = static_data_table.schema;
            let (attr_v, source_table, freq_column) = data_set
                .datumTemplates
                .iter()
                .filter(|x| {
                    x.get_name().clone() == t.datumTemplateName
                })
                .map(|x| match x {
                    DatumTemplate::IntegerMeasure(t) => (
                        t.get_attributes(),
                        t.source_asset_name.clone(),
                        t.get_frequency_attribute().get_name().clone(),
                     ),
                    _ => panic!("program for IntegerMeasure incorrectly called"),
                })
                .next().unwrap();
            let group_by_columns = attr_v.iter()
                .map(|x| format!("            {}",
                x.get_name().clone()).to_string())
                .collect::<Vec<String>>().join("\n, ");

            format!("
            INSERT INTO {presto_schema}.{table_name}
            SELECT
                {group_by_columns},
                COUNT(*) AS {freq_column}
            FROM {presto_schema}.{source_table_name}
            GROUP BY {group_by_columns}
            ",
                presto_schema=data_set.name,
                table_name=format!("{}", static_data_table.name),
                freq_column=freq_column,
                group_by_columns=group_by_columns,
                source_table_name=source_table,
            )
          }
---
type: Program
spec:
  use: SVMRegressionModelsTrained
  root: SupervisedModel
  dialect: Presto
  call: "{query}"
  kwargs:
    query:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - SupervisedModel
          - DataSet
        call: >
          {
            let template = data_set.get_template_for_asset(supervised_model);
            let tfm = match template {
                Ok(DatumTemplate::TrainedFloatMeasure(m)) => m,
                Err(x) => panic!(x),
                _ => panic!(format!(
                    "Cannot train an SVM Regression Model on asset with with template {}",
                    supervised_model.get_schema().get_datum_template_name(),
               )),
            };
            format!("
            CREATE TABLE {presto_schema}.tmp_{table_name}_regressor
            AS SELECT
                LEARN_REGRESSOR(
                    {objective},
                    FEATURES(
                        {features}
                    )
                ) AS {model_column_name}
            FROM {presto_schema}.{source_table_name}",
                source_table_name=tfm.source_asset_name,
                objective=tfm.get_training_objective().get_name().clone(),
                model_column_name=tfm.get_regressor_as_attribute().get_name().clone(),
                features=tfm.features.iter().map(|x| x.get_name().clone()).collect::<Vec<String>>().join(",\n              "),
                presto_schema=data_set.name,
                table_name=supervised_model.name,
            )
          }
---
type: Program
spec:
  use: ComputeFilter
  root: ComputedFromLocalData
  dialect: Presto
  call: "{query}"
  kwargs:
    query:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - DerivedAsset
          - DataSet
          - ComputedFromLocalData
        call: >
          {
            use crate::template::DatumTemplate;
            let DataSchema::TabularSchema(ref t) = derived_asset.schema;
            let mapped_templates = data_set.get_mapped_datum_templates();
            let template = mapped_templates.get(&t.datumTemplateName);
            let filter = match template {
                Some(DatumTemplate::Filter(m)) => m,
                _ => panic!(format!(
                    "Template {} not a filter.",
                    t.datumTemplateName
               )),
            };
            assert_eq!(computed_from_local_data.source_asset_names.len(), 1);
            let source_asset_name =
                computed_from_local_data.source_asset_names.values().next().unwrap().clone();
            let query = format!("
            INSERT INTO {presto_schema}.{table_name}
            SELECT {attributes}
            FROM {presto_schema}.{source_table_name}
            {predicate}",
                presto_schema=data_set.name,
                table_name=derived_asset.name,
                predicate=match filter.predicate {
                    Some(ref f) => format!("WHERE {}", f.as_sql()),
                    None => "".to_string(),
                },
                source_table_name=source_asset_name,
                attributes=
                    filter
                    .attributes
                    .iter()
                    .map(|x| x.get_name().clone())
                    .collect::<Vec<String>>()
                    .join(",\n"),
            ).to_string();
            let params = sqlformat::QueryParams::None;
            let options = sqlformat::FormatOptions {
                indent: sqlformat::Indent::Spaces(4),
                uppercase: true,
                lines_between_queries: 1
            };
            sqlformat::format(&query, &params, options).to_string()
          }
---
type: Program
spec:
  use: SVMRegressionModelsTrained
  root: SupervisedModel
  dialect: Python
  preamble: |
    import pandas as pd
    import pyarrow.orc as orc
    from minio import Minio
    import json
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType
    from sklearn.svm import SVR

    def download_and_train(
        minio,
        minio_prefix,
        tmp_dir,
        objective,
        features,
        asset_name,
        minio_output_path,
    ):

        (
            minio_hostname, minio_port, minio_access_key,
            minio_secret_key, minio_bucket
        ) = json.loads(minio)


        client = Minio(
            "%s:%s" % (minio_hostname, minio_port),
            access_key=minio_access_key,
            secret_key=minio_secret_key,
            secure=False,
        )

        assert client.bucket_exists(minio_bucket)
        objects = client.list_objects(minio_bucket, prefix=minio_prefix)
        datasets = []
        for i, obj in enumerate(objects):
            dest_file = '%s/%000d' % (tmp_dir, i)
            client.fget_object(
                minio_bucket,
                obj.object_name,
                file_path=dest_file,
            )

            with open(dest_file, 'rb') as f:
                data = orc.ORCFile(f)
                df = data.read().to_pandas()
                datasets += [df]
        data = pd.concat(datasets)

        features = json.loads(features)
        X = data[features].to_numpy()
        y = data[objective].to_numpy()
        svr = SVR(verbose=True)
        model = svr.fit(X, y)

        num_features = len(features)
        initial_type = [('float_input', FloatTensorType([None, num_features]))]
        onx = convert_sklearn(model, initial_types=initial_type)

        tmp_output_path = '%s/%s.onnx' % (tmp_dir, asset_name)
        with open(tmp_output_path, "wb") as f:
            f.write(onx.SerializeToString())
        client.fput_object(minio_bucket, minio_output_path, tmp_output_path)

  call: download_and_train
  kwargs:
    minio_output_path:
      type: AncestorArgument
      spec:
        attaches: SupervisedModel
        call: >
          {
            match supervised_model.setup {
                StorageSetup::ComputedFromLocalData(ref setup) => match
                setup.target {
                    Storage::LocalFileStorage(ref storage) => match
                    storage.location {
                        OnPremiseLocation::MinioLocation(ref l) => format!(
                            "{}/{}.onnx",
                            l.name,
                            supervised_model.get_name(),
                        ).to_string(),
                        _ => panic!("Only on-premise locations allowed."),
                    },
                    _ => panic!("Only local file storage allowed."),
                },
                _ => panic!("Only locally-computed models allowed."),
            }
          }

    objective:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - SupervisedModel
          - DataSet
        call: >
          {
            let template = data_set.get_template_for_asset(supervised_model);
            let tfm = match template {
                Ok(DatumTemplate::TrainedFloatMeasure(m)) => m,
                Err(x) => panic!(x),
                _ => panic!(format!(
                    "Cannot train an SVM Regression Model on asset with with template {}",
                    supervised_model.get_schema().get_datum_template_name(),
               )),
            };
            tfm.get_training_objective().get_name().clone()
          }
    asset_name:
      type: AncestorArgument
      spec:
        attaches: SupervisedModel
        call: supervised_model.get_name()
    features:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - SupervisedModel
          - DataSet
        call: >
          {
            let template = data_set.get_template_for_asset(supervised_model);
            let tfm = match template {
                Ok(DatumTemplate::TrainedFloatMeasure(m)) => m,
                Err(x) => panic!(x),
                _ => panic!(format!(
                    "Cannot train an SVM Regression Model on asset with with template {}",
                    supervised_model.get_schema().get_datum_template_name(),
               )),
            };
            format!("[{}]",
                tfm.features.iter().map(|x| format!("\"{}\"", x.get_name()).to_string()).collect::<Vec<String>>().join(","),
            ).to_string()
          }
    minio:
      type: AncestorArgument
      spec:
        attaches: Universe
        call: >
          {
            let endpoint_config = universe.endpoints.clone();
            let minio = endpoint_config.minio.unwrap();
            format!(
                "[\"{}\", {}, \"{}\", \"{}\", \"{}\"]",
                minio.server,
                minio.port,
                minio.access_key,
                minio.secret_key,
                minio.bucket,
            ).to_string()
          }
    tmp_dir:
      type: AncestorArgument
      spec:
        attaches: SupervisedModel
        call: supervised_model.setup.get_tmp_dir()
    minio_prefix:
      type: MultipleAncestorsArgument
      spec:
        call: |
            {
            let computed_from_local_data = match &supervised_model.setup {
                StorageSetup::ComputedFromLocalData(m) => m,
                _ => panic!("Only ComputedFromLocalData StorageSetups allowed here."),
            };
            assert_eq!(computed_from_local_data.source_asset_names.len(), 1);
            let source =
                computed_from_local_data.source_asset_names.values().next().unwrap().clone();
            let source_asset = data_set.get_asset(source.clone()).unwrap();
            let storage_setup = source_asset.get_storage_setup();
            let local = storage_setup.get_local_storage();
            let minio = local
              .into_iter()
              .map(
                |x| if let Storage::HiveTableStorage(hive) = x {
                  return Some(hive)
                } else {
                  return None
                }
              ).filter(|x| x.is_some())
              .map(
                |x| if let HiveLocation::MinioLocation(m) = x.unwrap().location {
                  return Some(m);
                } else {
                  return None;
                }
              )
              .filter(|x| x.is_some())
              .map(|x| x.unwrap())
              .next()
              .unwrap();
              format!("{}/{}/", minio.name, source)
            }
        attaches:
          - SupervisedModel
          - DataSet
---
type: Program
spec:
  use: ComputePredictions
  root: ComputedFromLocalData
  dialect: Python
  preamble: |
    import pandas as pd
    import pyarrow.orc as orc
    from minio import Minio
    import json
    import onnxruntime as rt
    import numpy

    def make_predictions(
        minio,
        tmp_dir,
        asset_name,
        source_path,
        model_path,
        destination_path,
        features,
    ):

        (
            minio_hostname, minio_port, minio_access_key,
            minio_secret_key, minio_bucket
        ) = json.loads(minio)
        client = Minio(
            "%s:%s" % (minio_hostname, minio_port),
            access_key=minio_access_key,
            secret_key=minio_secret_key,
            secure=False,
        )
        tmp_output_path = '%s/%s.onnx' % (tmp_dir, asset_name)
        client.fget_object(minio_bucket, model_path, tmp_output_path)

        sess = rt.InferenceSession(tmp_output_path)
        input_name = sess.get_inputs()[0].name
        label_name = sess.get_outputs()[0].name

        objects = client.list_objects(minio_bucket, prefix=source_path)
        datasets = []
        for i, obj in enumerate(objects):
            dest_file = '%s/%000d' % (tmp_dir, i)
            client.fget_object(
                minio_bucket,
                obj.object_name,
                file_path=dest_file,
            )

            with open(dest_file, 'rb') as f:
                data = orc.ORCFile(f)
                df = data.read().to_pandas()
                datasets += [df]

        input_data = pd.concat(datasets)

        features = json.loads(features)
        X = input_data[features].to_numpy()
        predictions = sess.run([label_name], {input_name: X.astype(numpy.float32)})[0]
        X = numpy.concatenate((X, predictions), axis=1)
        tmp_output_path = "%s/data.csv" % tmp_dir
        numpy.savetxt(tmp_output_path, X, delimiter=",")
        print("Wrote data.csv to %s" % destination_path)
        client.fput_object(minio_bucket, destination_path + "/data.csv", tmp_output_path)


  call: make_predictions
  kwargs:
    asset_name:
      type: AncestorArgument
      spec:
        attaches: ComputedFromLocalData
        call: computed_from_local_data.source_asset_names.values().next().unwrap().clone()
    minio:
      type: AncestorArgument
      spec:
        attaches: Universe
        call: >
          {
            let endpoint_config = universe.endpoints.clone();
            let minio = endpoint_config.minio.unwrap();
            format!(
                "[\"{}\", {}, \"{}\", \"{}\", \"{}\"]",
                minio.server,
                minio.port,
                minio.access_key,
                minio.secret_key,
                minio.bucket,
            ).to_string()
          }
    tmp_dir:
      type: AncestorArgument
      spec:
        attaches: ComputedFromLocalData
        call: computed_from_local_data.tmp_dir
    source_path:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - ComputedFromLocalData
          - DataSet
          - StaticDataTable
        call: |
          {
            let template = data_set.get_template_for_asset(static_data_table);
            let ptfm = match template {
                Ok(DatumTemplate::PredictionsFromTrainedFloatMeasure(x)) => x,
                _ => panic!("Template must be of type PredictionsFromTrainedFloatMeasure."),
            };
            let source_asset_role = ptfm.get_source_asset_role();

            assert_eq!(computed_from_local_data.source_asset_names.len(), 2);
            let source_name =
                computed_from_local_data.source_asset_names.get(&source_asset_role).unwrap();
            let source_asset = data_set.get_asset(source_name.clone()).unwrap();

            match source_asset {
              Asset::StaticDataTable(ref static_data_table) => {
                let local = static_data_table.setup.get_local_storage();
                assert_eq!(local.len(), 1);
                let local_storage = local.iter().next().unwrap();
                match local_storage {
                      Storage::HiveTableStorage(ref storage) => match
                      storage.location {
                          HiveLocation::MinioLocation(ref l) =>
                            format!("{}/{}/", l.name, static_data_table.get_name()).to_string(),
                          _ => panic!("Only on-premise locations allowed."),
                      },
                      _ => panic!("Only HiveTableStorage allowed."),
                }
              }
              _ => panic!(format!(
                  "Source asset {} must be StaticDataTable", source_name,
              )),
            }
          }
    model_path:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - ComputedFromLocalData
          - DataSet
          - StaticDataTable
        call: |
          {
            let template = data_set.get_template_for_asset(static_data_table);
            let ptfm = match template {
                Ok(DatumTemplate::PredictionsFromTrainedFloatMeasure(x)) => x,
                _ => panic!("Template must be of type PredictionsFromTrainedFloatMeasure."),
            };
            let model_asset_role = ptfm.get_model_asset_role();

            assert_eq!(computed_from_local_data.source_asset_names.len(), 2);
            let model_name =
                computed_from_local_data.source_asset_names.get(&model_asset_role).unwrap();
            let source_asset = data_set.get_asset(model_name.clone()).unwrap();

            match source_asset {
              Asset::SupervisedModel(ref supervised_model) =>
              match supervised_model.setup {
                  StorageSetup::ComputedFromLocalData(ref setup) => match
                  setup.target {
                      Storage::LocalFileStorage(ref storage) => match
                      storage.location {
                          OnPremiseLocation::MinioLocation(ref l) => format!(
                              "{}/{}.onnx",
                              l.name,
                              supervised_model.get_name(),
                          ).to_string(),
                          _ => panic!("Only on-premise locations allowed."),
                      },
                      _ => panic!("Only local file storage allowed."),
                  },
                  _ => panic!("Only locally-computed models allowed."),
              },
              _ => panic!(format!(
                  "Model asset {} must be supervised model", model_name,
              )),
            }
          }
    destination_path:
      type: AncestorArgument
      spec:
        attaches: StaticDataTable
        call: >
          {
            match static_data_table.setup {
                StorageSetup::ComputedFromLocalData(ref setup) => match
                setup.target {
                    Storage::HiveTableStorage(ref storage) => match
                    storage.location {
                        HiveLocation::MinioLocation(ref l) => format!(
                            "{}/{}_csv",
                            l.name,
                            static_data_table.get_name(),
                        ).to_string(),
                        _ => panic!("Only on-premise locations allowed."),
                    },
                    _ => panic!("Only HiveTable storage allowed."),
                },
                _ => panic!("Only StaticDataTable assets allowed."),
            }
          }
    features:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - DataSet
          - StaticDataTable
        call: >
          {
            let template = data_set.get_template_for_asset(static_data_table);
            let ptfm = match template {
                Ok(DatumTemplate::PredictionsFromTrainedFloatMeasure(x)) => x,
                _ => panic!("Template must be of type PredictionsFromTrainedFloatMeasure."),
            };
            format!("[{}]",
                ptfm.features.iter().map(|x| format!("\"{}\"", x.get_name()).to_string()).collect::<Vec<String>>().join(","),
            ).to_string()
          }
---
type: Program
spec:
  use: ConvertPredictionsCSVTableToORCTable
  root: HiveTableStorage
  dialect: Presto
  call: \nINSERT INTO {presto_schema}.{table_name} \nSELECT {columns} \nFROM {presto_schema}.{table_name}_csv\n
  kwargs:
    table_name:
      type: AncestorArgument
      spec:
        call: format!("{}", static_data_table.name).to_string()
        attaches: StaticDataTable
    presto_schema:
      type: AncestorArgument
      spec:
        attaches: DataSet
        call: data_set.name.clone()
    columns:
      type: MultipleAncestorsArgument
      spec:
        call: >
          {
            use crate::template::TDatumTemplate;
            let DataSchema::TabularSchema(ref t) = static_data_table.schema;
            let attr_v = data_set
                .datumTemplates
                .iter()
                .filter(|x| {
                    x.get_name().clone() == t.datumTemplateName
                })
                .map(|x| x.get_attributes())
                .next().unwrap();
            let attr_map = attr_v.iter()
                .map(|x|
                    (x.get_name().clone(), x.get_presto_type()))
                .collect::<Vec<_>>();
            let attr_types = attr_map.iter().map(|x| {
                let (name, presto_type) = x;
                return format!(
                    "CAST({name} AS {presto_type}) AS {name}",
                    name=name,
                    presto_type=presto_type,
                ).to_string();
            }).collect::<Vec<String>>();
            format!("\n{}", attr_types.join(",\n"))
          }
        attaches:
          - StaticDataTable
          - DataSet
---
type: Program
spec:
  use: UploadDataToSQLite
  root: SQLiteLocation
  dialect: Python
  preamble: |
    def upload_to_sqlite(
        db_filename, tablename, tmp_dir, source_file,
        # insertion order matters if we are dealing with a csv
        columns,
        source_is_json,
    ):
        import sqlite3
        import json
        source_is_json = bool(source_is_json)
        assert source_is_json or header is None

        columns = json.loads(columns)

        con = sqlite3.connect(db_filename)
        con.execute("""
        CREATE TABLE {tablename}({schema})
        """.format(
            tablename=tablename,
            schema=",\\n".join(["%s %s" % (k, v) for k, v in columns]),
        ))

        values = []
        type_fn = []
        for _, v in columns:
            if v == 'TEXT':
                type_fn += [lambda x: x]
            elif v == 'INTEGER':
                type_fn += [int]
            elif v == 'REAL':
                type_fn += [float]
            elif v == 'BLOB':
                type_fn += [bytes]
            else:
                type_fn += [lambda _: None]
        
        with open(source_file, 'r') as f:
            if source_is_json:
                for line in f.readlines():
                    obj = json.loads(line)
                    tpl = tuple(
                        fn(obj[x])
                        for fn, x in zip(
                            type_fn,
                            [k for k, _ in columns]
                        )
                    )
                    values += [tpl]
            else:
                for line in f.readlines():
                    splits = line.strip().split(",")
                    assert len(splits) == len(type_fn)
                    tpl = tuple(fn(arg) for fn, arg in zip(type_fn, splits))
                    values += [tpl]
        
        con.executemany(
            """
                INSERT INTO {tablename}({columns}) VALUES ({vals})
            """.format(
                tablename=tablename,
                columns=", ".join([k for k, _ in columns]),
                vals=", ".join(["?"] * len(columns))
            ), 
            values
        )
  call: upload_to_sqlite
  kwargs:
    db_filename:
      type: AncestorArgument
      spec:
        call: sq_lite_location.file_name
        attaches: SQLiteLocation
    tablename:
      type: MultipleAncestorsArgument
      spec:
        call: format!("{}__{}", data_set.name, static_data_table.name).to_string()
        attaches: 
          - StaticDataTable
          - DataSet
    tmp_dir:
      type: AncestorArgument
      spec:
        call: remote_import_storage_setup.tmp_dir
        attaches: RemoteImportStorageSetup
    source_file:
      type: AncestorArgument
      spec:
        call: format!("{}.csv", static_data_table.name).to_string()
        attaches: StaticDataTable
    columns:
      type: MultipleAncestorsArgument
      spec:
        attaches:
          - DataSet
          - StaticDataTable
        call: >
          {
            use crate::template::TDatumTemplate;
            let template =
            data_set.get_template_for_asset(static_data_table).unwrap();
            let attributes = template.get_attributes();
            let attributes_vec = attributes.into_iter().map(
                |x| (x.get_name().to_string(), x.get_sql_type().to_string())
            ).collect::<Vec<_>>();
            serde_json::json!(attributes_vec).to_string()
          }
    source_is_json:
      type: AncestorArgument
      spec:
        attaches: StorageSetup
        call: >
          format!("{}", match storage_setup {
            crate::StorageSetup::RemoteImportStorageSetup(r) =>
              match r.remote.get_encoding() {
                Some(crate::Encoding::JSONEncoding(_)) => "True",
                _ => "False",
              },
              _ => "False",
          }).to_string()
