---
type: Constraint
spec:
  name: Replicated
  root: Universe
  requires:
    - ReplicatedDataSets
---
type: Constraint
spec:
  name: ReplicatedDataSets
  root: DataSet
  requires:
    - ReplicatedAssets
---
type: Constraint
spec:
  name: ReplicatedAssets
  root: Asset
  requires:
    - ReplicatedSchema
---
type: Constraint
spec:
  name: ReplicatedSchema
  root: StaticDataTable
  requires:
      - TableSchemasCreated
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      match ancestry.static_data_table(root.clone()).unwrap().setup {
          crate::storage_setup::StorageSetup::RemoteImportStorageSetup(_) => true,
          _ => false,
      }
---
type: Constraint
spec:
  name: DataDownloadedAndConverted
  root: Universe
  requires:
    - DroppedCSVTable
---
type: Constraint
spec:
  name: TableSchemasCreated
  root: HiveTableStorage
  requiresProgram: true
  requires:
    - TableSchemasDroppedIfExisting
  title: Creating Table Schemas
  body: |
      We will be uploading tabular data into our warehouse. Before we upload
      data files we need to create schemas for the tables which will refer
      to these files.
---
type: Constraint
spec:
  name: TableSchemasDroppedIfExisting
  root: HiveTableStorage
  requiresProgram: true
  requires:
    - HiveDirectoriesCreated
  title: Drop table schemas if they already exist.
  body: |
      Beware! Our workflow will overwrite any existing table schemas with the same
      name as our data.
---
type: Constraint
spec:
  name: CSVTableSchemasCreated
  root: HiveTableStorage
  requiresProgram: true
  requires:
    - HiveDirectoriesCreated
    - UploadDataToLocal
  title: Create schemas for temporary CSV tables.
  body: |
      We will use Hive tables with external storage as a staging location for our
      data. We need to create these schemas to be able to write data to them.
---
type: Constraint
spec:
  name: HiveDirectoriesCreated
  root: HiveLocation
  requiresProgram: true
  title: Created hive directories.
  body: |
      We need to create directories or buckets (depending on file system / storage
      solution) in which we will store our Hive data.
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      ancestry.remote_import_storage_setup(root.clone()).is_ok()
---
type: Constraint
spec:
  name: IsAuditedTable
  root: StaticDataTable
  requires:
    - Replicated
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      match ancestry.static_data_table(root.clone()).unwrap().setup {
          crate::storage_setup::StorageSetup::RemoteImportStorageSetup(_) => true,
          _ => false,
      }
---
type: Constraint
spec:
  name: IsAudited
  root: Universe
  requires:
    - IsAuditedTable
---
type: Constraint
spec:
  name: ReplicatedData
  root: RemoteImportStorageSetup
  requires:
    - UploadDataToLocal
    - FileReadyForUpload
---
type: Constraint
spec:
  name: FileReadyForUpload
  root: RemoteImportStorageSetup
  requires:
    - RemoveFileHeader
---
type: Constraint
spec:
  name: DownloadDataFromRemote
  root: RemoteStorage
  requires:
    - DownloadDataFromRemoteGCSLocation
    - DownloadDataFromRemoteWebLocation
---
type: Constraint
spec:
  name: DownloadDataFromRemoteGCSLocation
  root: GCSLocation
  requiresProgram: true
  title: Downloading data from GCS
  body: |
      Data for this particular asset(s) is located in Google Cloud Storage.
      We need to download it to a local directory first, before we can
      do anything with it.
---
type: Constraint
spec:
  name: DownloadDataFromRemoteWebLocation
  root: WebLocation
  requiresProgram: true
  title: Downloading data from remote web location
  body: |
      Data for this particular asset(s) is located somewhere on the web.
      We need to download it to a local directory first, before we can
      do anything with it.
---
type: Constraint
spec:
  name: RemoveFileHeader
  root: FileHeader
  requiresProgram: true
  requires:
    - FileIsDecompressed
  title: Removing file header
  body: |
      We are dealing with a tabular file with a header. Before we can
      process it we need to remove the header.
---
type: Constraint
spec:
  name: FileIsDecompressed
  root: RemoteStorage
  requires:
    - Decompress
---
type: Constraint
spec:
  name: Decompress
  root: DataCompression
  requires:
    - DecompressGzip
    - DecompressZip
---
type: Constraint
spec:
  name: DecompressGzip
  root: GzipCompression
  requiresProgram: true
  requires:
    - DownloadDataFromRemote
  title: Decompressing Gzip file
  body: |
      Data for this particular asset(s) is compressed with the GZIP
      algorith. Before we can process it further we need to decompress it.
---
type: Constraint
spec:
  name: DecompressZip
  root: ZipCompression
  requiresProgram: true
  requires:
    - DownloadDataFromRemote
  title: Decompressing Zipped file
  body: |
      Data for this particular asset(s) is compressed with the Zip
      algorith. Before we can process it further we need to decompress it.
---
type: Constraint
spec:
  name: ConvertCSVToOrc
  root: RemoteImportStorageSetup
  requiresProgram: true
  title: Convert CSV to ORC
  body: |
      The remote data source for asset is in CSV format, but we need to
      convert it to ORC format (native Hive table format) to be usable
      at the destination.
---
type: Constraint
spec:
  name: UploadDataToLocal
  root: HiveLocation
  requires:
      - UploadDataToAlluxio
      - UploadDataToMinio
---
type: Constraint
spec:
  name: UploadDataToAlluxio
  root: AlluxioLocation
  requiresProgram: true
  requires:
    - CSVIsConverted
    - ReplicatedSchema
  title: Upload data to Alluxio
  body: |
      Now that data has been pre-processed we can upload it to the underlying
      Alluxio storage.
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      ancestry.remote_import_storage_setup(root.clone()).is_ok()
---
type: Constraint
spec:
  name: UploadDataToMinio
  root: MinioLocation
  requiresProgram: true
  requires:
    - CSVIsConverted
    - ReplicatedSchema
  title: Upload data to Min.IO
  body: |
      Now that data has been pre-processed we can upload it to the underlying
      Alluxio storage.
---
type: Constraint
spec:
  name: CSVIsConverted
  root: StaticDataTable
  requires:
    - ConvertToCSV
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      match ancestry.static_data_table(root.clone()).unwrap().setup {
          crate::storage_setup::StorageSetup::RemoteImportStorageSetup(_) => true,
          _ => false,
      }
---
type: Constraint
spec:
  name: ConvertToCSV
  root: RemoteStorage
  requiresProgram: true
  requires:
    - RemoveFileHeader
  title: Convert data to CSV
  body: |
      We need to convert the data to CSV format to process it further (or if
      the data is already in CSV format, we will need to rename files accordingly).
  attachIf: |
      |root: Concept<'a>, ancestry: &ConceptAncestry<'a>|
      ancestry.remote_import_storage_setup(root.clone()).is_ok()
---
type: Constraint
spec:
  name: ConvertCSVTableToORCTable
  root: HiveTableStorage
  requires:
    - CSVTableSchemasCreated
  requiresProgram: true
  title: Convert CSV Table to ORC Table
  body: |
      Hive tables can be stored in external CSV format, but this is inefficient.
      We can convert them to ORC (the native Hive format) to speed up access.
---
type: Constraint
spec:
  name: DroppedCSVTable
  root: HiveTableStorage
  requires:
    - ConvertCSVTableToORCTable
  requiresProgram: true
  title: Drop CSV Table
  body: |
      After we've converted a CSV table to ORC the CSV table is no longer needed,
      so it can be dropped.
